{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6013987",
   "metadata": {},
   "source": [
    "# 欠損値補完モデルで, testデータの欠損値を保管したデータにて学習\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fef6196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "データを読み込んでいます: ../data/all_data_imputed_step7_Spending.csv\n",
      "データ読み込み完了!\n",
      "\n",
      "--- データセットの基本情報 ---\n",
      "データ形状 (行, 列): (12970, 16)\n",
      "\n",
      "列名:\n",
      "['PassengerId', 'HomePlanet', 'CryoSleep', 'Cabin', 'Destination', 'Age', 'VIP', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'Name', 'Transported', 'Deck', 'Side']\n",
      "\n",
      "データ型:\n",
      "PassengerId      object\n",
      "HomePlanet       object\n",
      "CryoSleep        object\n",
      "Cabin            object\n",
      "Destination      object\n",
      "Age             float64\n",
      "VIP                bool\n",
      "RoomService     float64\n",
      "FoodCourt       float64\n",
      "ShoppingMall    float64\n",
      "Spa             float64\n",
      "VRDeck          float64\n",
      "Name             object\n",
      "Transported      object\n",
      "Deck             object\n",
      "Side             object\n",
      "dtype: object\n",
      "\n",
      "先頭5行:\n",
      "  PassengerId HomePlanet CryoSleep  Cabin  Destination   Age    VIP  \\\n",
      "0     0001_01     Europa     False  B/0/P  TRAPPIST-1e  39.0  False   \n",
      "1     0002_01      Earth     False  F/0/S  TRAPPIST-1e  24.0  False   \n",
      "2     0003_01     Europa     False  A/0/S  TRAPPIST-1e  58.0   True   \n",
      "3     0003_02     Europa     False  A/0/S  TRAPPIST-1e  33.0  False   \n",
      "4     0004_01      Earth     False  F/1/S  TRAPPIST-1e  16.0  False   \n",
      "\n",
      "   RoomService  FoodCourt  ShoppingMall     Spa  VRDeck               Name  \\\n",
      "0          0.0        0.0           0.0     0.0     0.0    Maham Ofracculy   \n",
      "1        109.0        9.0          25.0   549.0    44.0       Juanna Vines   \n",
      "2         43.0     3576.0           0.0  6715.0    49.0      Altark Susent   \n",
      "3          0.0     1283.0         371.0  3329.0   193.0       Solam Susent   \n",
      "4        303.0       70.0         151.0   565.0     2.0  Willy Santantines   \n",
      "\n",
      "  Transported Deck Side  \n",
      "0       False    B    P  \n",
      "1        True    F    S  \n",
      "2       False    A    S  \n",
      "3       False    A    S  \n",
      "4        True    F    S  \n",
      "\n",
      "欠損値の確認:\n",
      "Name            294\n",
      "Transported    4277\n",
      "dtype: int64\n",
      "\n",
      "--- 整合性チェック ---\n",
      "元の test.csv の行数: 4277\n",
      "all_data から分離した test_df の行数: 4277\n",
      "✓ 整合性チェック OK: test.csv の PassengerId と分離された test_df の PassengerId は一致しています。\n",
      "\n",
      "--- 学習データとテストデータの分割 ---\n",
      "学習データ形状: (8693, 16)\n",
      "テストデータ形状: (4277, 16)\n",
      "学習データの目的変数 'Transported' の分布:\n",
      "Transported\n",
      "True     4378\n",
      "False    4315\n",
      "Name: count, dtype: int64\n",
      "\n",
      "--- セル1完了 ---\n",
      "次のステップ: 特徴量エンジニアリングと前処理\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gohan/miniforge3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# セル 1: ライブラリのインポートとデータの読み込み・確認\n",
    "\n",
    "# --- 1. 必要なライブラリのインポート ---\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.model_selection import train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# --- 2. データの読み込み ---\n",
    "# 指定されたパスから最終補完データを読み込む\n",
    "final_data_path = '../data/all_data_imputed_step7_Spending.csv' # パスは環境に応じて調整してください\n",
    "print(f\"データを読み込んでいます: {final_data_path}\")\n",
    "all_data = pd.read_csv(final_data_path)\n",
    "print(\"データ読み込み完了!\")\n",
    "\n",
    "# --- 3. データの基本情報確認 ---\n",
    "print(\"\\n--- データセットの基本情報 ---\")\n",
    "print(f\"データ形状 (行, 列): {all_data.shape}\")\n",
    "print(\"\\n列名:\")\n",
    "print(all_data.columns.tolist())\n",
    "print(\"\\nデータ型:\")\n",
    "print(all_data.dtypes)\n",
    "print(\"\\n先頭5行:\")\n",
    "print(all_data.head())\n",
    "print(\"\\n欠損値の確認:\")\n",
    "missing_values = all_data.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# --- 4. 学習データとテストデータの分離 (Transported の有無で判断) ---\n",
    "# Transported が NaN でない行が学習データ\n",
    "train_df = all_data[all_data['Transported'].notna()].copy()\n",
    "# Transported が NaN の行がテストデータ\n",
    "test_df = all_data[all_data['Transported'].isna()].copy()\n",
    "\n",
    "# 目的変数の型を bool に変換 (学習データのみ)\n",
    "train_df['Transported'] = train_df['Transported'].astype(bool)\n",
    "\n",
    "# --- 5. 整合性チェック: test.csv の PassengerId と all_data 内のテストデータ PassengerId が一致するか ---\n",
    "print(\"\\n--- 整合性チェック ---\")\n",
    "original_test_path = '../data/test.csv' # 元の test.csv のパス\n",
    "original_test_df = pd.read_csv(original_test_path)\n",
    "print(f\"元の test.csv の行数: {len(original_test_df)}\")\n",
    "print(f\"all_data から分離した test_df の行数: {len(test_df)}\")\n",
    "\n",
    "# PassengerId の集合を比較\n",
    "set_original_test_ids = set(original_test_df['PassengerId'])\n",
    "set_separated_test_ids = set(test_df['PassengerId'])\n",
    "\n",
    "if set_original_test_ids == set_separated_test_ids:\n",
    "    print(\"✓ 整合性チェック OK: test.csv の PassengerId と分離された test_df の PassengerId は一致しています。\")\n",
    "elif set_separated_test_ids.issuperset(set_original_test_ids):\n",
    "    diff = set_separated_test_ids - set_original_test_ids\n",
    "    print(f\"! 注意: 分離された test_df に余分な PassengerId があります: {sorted(list(diff))[:5]}...\") # 最初の5つだけ表示\n",
    "else:\n",
    "    diff = set_original_test_ids - set_separated_test_ids\n",
    "    print(f\"✗ 整合性チェック 失敗: test.csv に含まれるが分離された test_df にない PassengerId があります: {sorted(list(diff))[:5]}...\") # 最初の5つだけ表示\n",
    "\n",
    "\n",
    "# --- 6. 分割後のデータ確認 ---\n",
    "print(\"\\n--- 学習データとテストデータの分割 ---\")\n",
    "print(f\"学習データ形状: {train_df.shape}\")\n",
    "print(f\"テストデータ形状: {test_df.shape}\")\n",
    "print(f\"学習データの目的変数 'Transported' の分布:\")\n",
    "print(train_df['Transported'].value_counts(dropna=False))\n",
    "\n",
    "\n",
    "# 次のステップに進む準備ができました。\n",
    "print(\"\\n--- セル1完了 ---\")\n",
    "print(\"次のステップ: 特徴量エンジニアリングと前処理\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9e729d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 基本特徴量の抽出 ---\n",
      "学習特徴量形状: (8693, 12)\n",
      "テスト特徴量形状: (4277, 12)\n",
      "目的変数形状: (8693,)\n",
      "\n",
      "--- エンコーディング対象の特徴量 ---\n",
      "['HomePlanet', 'CryoSleep', 'Destination', 'Age', 'VIP', 'Deck', 'Side', 'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck', 'TotalSpending', 'LogTotalSpending', 'GroupSize', 'AgeBand', 'PlanetRoute', 'SpendingPerAge']\n",
      "\n",
      "結合後特徴量形状: (12970, 18)\n",
      "\n",
      "--- エンコード後のデータ形状 ---\n",
      "学習特徴量 (X_train_processed) 形状: (8693, 46)\n",
      "テスト特徴量 (X_test_processed) 形状: (4277, 46)\n",
      "目的変数 (y_train) 形状: (8693,)\n",
      "\n",
      "最終特徴量数: 46\n",
      "\n",
      "--- セル2完了 ---\n",
      "次のステップ: モデルの選定・学習・評価\n"
     ]
    }
   ],
   "source": [
    "# セル 2: 特徴量エンジニアリング (Feature Engineering)\n",
    "\n",
    "# --- 1. 基本特徴量の選択 ---\n",
    "feature_columns_base = [\n",
    "    'HomePlanet', 'CryoSleep', 'Destination', 'Age', 'VIP',\n",
    "    'Deck', 'Side',\n",
    "    'RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck'\n",
    "]\n",
    "\n",
    "# 学習データとテストデータから基本特徴量を抽出\n",
    "X_train_base = train_df[feature_columns_base].copy()\n",
    "X_test_base = test_df[feature_columns_base].copy()\n",
    "y_train = train_df['Transported'].copy() # 目的変数\n",
    "\n",
    "print(\"--- 基本特徴量の抽出 ---\")\n",
    "print(f\"学習特徴量形状: {X_train_base.shape}\")\n",
    "print(f\"テスト特徴量形状: {X_test_base.shape}\")\n",
    "print(f\"目的変数形状: {y_train.shape}\")\n",
    "\n",
    "# --- 2. 派生特徴量の作成 ---\n",
    "# --- 2.1. Total Spending ---\n",
    "spending_columns = ['RoomService', 'FoodCourt', 'ShoppingMall', 'Spa', 'VRDeck']\n",
    "X_train_base['TotalSpending'] = X_train_base[spending_columns].sum(axis=1)\n",
    "X_test_base['TotalSpending'] = X_test_base[spending_columns].sum(axis=1)\n",
    "\n",
    "# --- 2.2. Log Total Spending (0を除外) ---\n",
    "X_train_base['LogTotalSpending'] = np.log1p(X_train_base['TotalSpending']) # log1p は 0 を扱える\n",
    "X_test_base['LogTotalSpending'] = np.log1p(X_test_base['TotalSpending'])\n",
    "\n",
    "# --- 2.3. GroupID (PassengerId から抽出) ---\n",
    "X_train_base['GroupID'] = train_df['PassengerId'].str.split('_').str[0]\n",
    "X_test_base['GroupID'] = test_df['PassengerId'].str.split('_').str[0]\n",
    "\n",
    "# --- 2.4. GroupSize (同じ GroupID の人数) ---\n",
    "# GroupID の出現回数をカウント\n",
    "group_counts_train = X_train_base['GroupID'].value_counts().to_dict()\n",
    "group_counts_test = X_test_base['GroupID'].value_counts().to_dict()\n",
    "\n",
    "# 訓練データとテストデータの GroupID カウントをマージ (GroupID が完全に別ならこれでOK)\n",
    "# より正確には、all_data でカウントするのがベストだが、簡略化\n",
    "X_train_base['GroupSize'] = X_train_base['GroupID'].map(group_counts_train)\n",
    "X_test_base['GroupSize'] = X_test_base['GroupID'].map(group_counts_test)\n",
    "\n",
    "# --- 2.5. AgeBand (年齢層ビン分け) ---\n",
    "# ビンの境界を定義 (例: 0-12, 13-17, 18-25, 26-30, 31-50, 51+)\n",
    "bins = [0, 12, 17, 25, 30, 50, 100]\n",
    "labels = ['Child', 'Teen', 'Young_Adult', 'Adult', 'Mid_Age', 'Senior']\n",
    "X_train_base['AgeBand'] = pd.cut(X_train_base['Age'], bins=bins, labels=labels, right=False)\n",
    "X_test_base['AgeBand'] = pd.cut(X_test_base['Age'], bins=bins, labels=labels, right=False)\n",
    "\n",
    "# --- 2.6. PlanetRoute (HomePlanet x Destination) ---\n",
    "X_train_base['PlanetRoute'] = X_train_base['HomePlanet'].astype(str) + \"_to_\" + X_train_base['Destination'].astype(str)\n",
    "X_test_base['PlanetRoute'] = X_test_base['HomePlanet'].astype(str) + \"_to_\" + X_test_base['Destination'].astype(str)\n",
    "\n",
    "# --- 2.7. SpendingPerAge (例: TotalSpending / Age, Age>0) ---\n",
    "# np.where を使って、Ageが0の場合は NaN または 0 にするなどの処理を入れる\n",
    "X_train_base['SpendingPerAge'] = np.where(X_train_base['Age'] > 0, X_train_base['TotalSpending'] / X_train_base['Age'], 0)\n",
    "X_test_base['SpendingPerAge'] = np.where(X_test_base['Age'] > 0, X_test_base['TotalSpending'] / X_test_base['Age'], 0)\n",
    "\n",
    "# --- 3. エンコーディングの準備 ---\n",
    "# GroupID は CV 分割にのみ使用し、モデルには入力しない\n",
    "features_for_encoding = X_train_base.columns.drop('GroupID').tolist()\n",
    "print(f\"\\n--- エンコーディング対象の特徴量 ---\")\n",
    "print(features_for_encoding)\n",
    "\n",
    "# 学習データとテストデータを結合して、一貫したエンコーディングを行う\n",
    "combined_features_raw = pd.concat([X_train_base[features_for_encoding], X_test_base[features_for_encoding]], axis=0, ignore_index=True)\n",
    "print(f\"\\n結合後特徴量形状: {combined_features_raw.shape}\")\n",
    "\n",
    "# --- 4. One-Hot Encoding ---\n",
    "# カテゴリ変数を指定 (数値型以外 or 明示的に指定)\n",
    "categorical_features = ['HomePlanet', 'CryoSleep', 'Destination', 'Deck', 'Side', 'AgeBand', 'PlanetRoute']\n",
    "# VIP は bool 型なので、必要に応じて変換 (ここではそのまま bool として扱う)\n",
    "\n",
    "# One-Hot Encoding を適用 (dummy_na=True で NaN もカテゴリとして扱うが、今回のデータでは欠損なし)\n",
    "combined_features_encoded = pd.get_dummies(combined_features_raw, columns=categorical_features, dummy_na=False, drop_first=False)\n",
    "\n",
    "# --- 5. エンコード後のデータを学習用とテスト用に再分割 ---\n",
    "X_train_processed = combined_features_encoded.iloc[:len(X_train_base)].reset_index(drop=True)\n",
    "X_test_processed = combined_features_encoded.iloc[len(X_train_base):].reset_index(drop=True)\n",
    "\n",
    "# 目的変数 y_train もリセットインデックス\n",
    "y_train = y_train.reset_index(drop=True)\n",
    "\n",
    "print(f\"\\n--- エンコード後のデータ形状 ---\")\n",
    "print(f\"学習特徴量 (X_train_processed) 形状: {X_train_processed.shape}\")\n",
    "print(f\"テスト特徴量 (X_test_processed) 形状: {X_test_processed.shape}\")\n",
    "print(f\"目的変数 (y_train) 形状: {y_train.shape}\")\n",
    "\n",
    "# --- 6. 使用する特徴量カラムの最終確認 ---\n",
    "final_feature_columns = X_train_processed.columns.tolist()\n",
    "print(f\"\\n最終特徴量数: {len(final_feature_columns)}\")\n",
    "# print(\"最終特徴量カラム名:\")\n",
    "# print(final_feature_columns) # 長いので必要時のみコメントアウトを外す\n",
    "\n",
    "print(\"\\n--- セル2完了 ---\")\n",
    "print(\"次のステップ: モデルの選定・学習・評価\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23873dc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 23:13:15,427] A new study created in memory with name: no-name-fe666aaa-d944-4c96-b830-7fb07ee41238\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna チューニングを開始します...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 23:13:18,025] Trial 0 finished with value: 0.802193521502045 and parameters: {'lambda_l1': 0.007598412497576287, 'lambda_l2': 0.04462337491026621, 'num_leaves': 143, 'feature_fraction': 0.9394483708078196, 'bagging_fraction': 0.46780270170600136, 'bagging_freq': 1, 'min_child_samples': 39, 'min_gain_to_split': 4.595296993001769, 'learning_rate': 0.052536767526617095}. Best is trial 0 with value: 0.802193521502045.\n",
      "[I 2025-07-25 23:13:23,383] Trial 1 finished with value: 0.8042699600916954 and parameters: {'lambda_l1': 5.653250933369594e-06, 'lambda_l2': 3.11227845169353e-06, 'num_leaves': 255, 'feature_fraction': 0.6316752717927361, 'bagging_fraction': 0.8809764022479475, 'bagging_freq': 1, 'min_child_samples': 59, 'min_gain_to_split': 2.568817254841656, 'learning_rate': 0.010459067964705627}. Best is trial 1 with value: 0.8042699600916954.\n",
      "[I 2025-07-25 23:13:26,011] Trial 2 finished with value: 0.8008878268279478 and parameters: {'lambda_l1': 5.638342677328735e-08, 'lambda_l2': 5.995566752168143e-05, 'num_leaves': 175, 'feature_fraction': 0.47342364303115453, 'bagging_fraction': 0.8677304030308678, 'bagging_freq': 5, 'min_child_samples': 61, 'min_gain_to_split': 3.9620911484024326, 'learning_rate': 0.05555135315638356}. Best is trial 1 with value: 0.8042699600916954.\n",
      "[I 2025-07-25 23:13:27,176] Trial 3 finished with value: 0.8001687999010663 and parameters: {'lambda_l1': 0.37997274317249363, 'lambda_l2': 1.24112436873327e-06, 'num_leaves': 217, 'feature_fraction': 0.5247201681852838, 'bagging_fraction': 0.5610270577991849, 'bagging_freq': 4, 'min_child_samples': 8, 'min_gain_to_split': 1.9498754812951458, 'learning_rate': 0.27525964824973415}. Best is trial 1 with value: 0.8042699600916954.\n",
      "[I 2025-07-25 23:13:32,457] Trial 4 finished with value: 0.803826848899643 and parameters: {'lambda_l1': 0.11561540284607619, 'lambda_l2': 6.400371893303237e-08, 'num_leaves': 31, 'feature_fraction': 0.6646801105161035, 'bagging_fraction': 0.4958777191401417, 'bagging_freq': 1, 'min_child_samples': 53, 'min_gain_to_split': 3.267983237180499, 'learning_rate': 0.01987279187650431}. Best is trial 1 with value: 0.8042699600916954.\n",
      "[I 2025-07-25 23:13:40,191] Trial 5 finished with value: 0.8020625985072962 and parameters: {'lambda_l1': 0.0002706150304437378, 'lambda_l2': 0.000667608218103054, 'num_leaves': 82, 'feature_fraction': 0.9190899863486309, 'bagging_fraction': 0.9785815462938241, 'bagging_freq': 3, 'min_child_samples': 26, 'min_gain_to_split': 4.8796189541275385, 'learning_rate': 0.011806365204272456}. Best is trial 1 with value: 0.8042699600916954.\n",
      "[I 2025-07-25 23:13:42,454] Trial 6 finished with value: 0.8015304507036266 and parameters: {'lambda_l1': 0.04234019529136727, 'lambda_l2': 0.04898935122104031, 'num_leaves': 157, 'feature_fraction': 0.5013976735510508, 'bagging_fraction': 0.6215168365292232, 'bagging_freq': 7, 'min_child_samples': 10, 'min_gain_to_split': 4.432485325557337, 'learning_rate': 0.033806211734512905}. Best is trial 1 with value: 0.8042699600916954.\n",
      "[I 2025-07-25 23:13:53,603] Trial 7 finished with value: 0.7896181432502243 and parameters: {'lambda_l1': 0.7548602391414408, 'lambda_l2': 0.010196428986751193, 'num_leaves': 95, 'feature_fraction': 0.4463408585067634, 'bagging_fraction': 0.9237260823139225, 'bagging_freq': 2, 'min_child_samples': 70, 'min_gain_to_split': 1.790362886110739, 'learning_rate': 0.0011901007892887805}. Best is trial 1 with value: 0.8042699600916954.\n",
      "[I 2025-07-25 23:14:12,134] Trial 8 finished with value: 0.8021345441606671 and parameters: {'lambda_l1': 0.049482785205101916, 'lambda_l2': 2.840815307868589, 'num_leaves': 166, 'feature_fraction': 0.9982249562480806, 'bagging_fraction': 0.8061385802809908, 'bagging_freq': 6, 'min_child_samples': 12, 'min_gain_to_split': 0.01090970389700463, 'learning_rate': 0.026278651225399303}. Best is trial 1 with value: 0.8042699600916954.\n",
      "[I 2025-07-25 23:14:13,124] Trial 9 finished with value: 0.8024460963053199 and parameters: {'lambda_l1': 0.09859863747976975, 'lambda_l2': 0.00013670732760740167, 'num_leaves': 155, 'feature_fraction': 0.7868116776770195, 'bagging_fraction': 0.9080898139438297, 'bagging_freq': 5, 'min_child_samples': 90, 'min_gain_to_split': 2.746800040152543, 'learning_rate': 0.2782105720765791}. Best is trial 1 with value: 0.8042699600916954.\n",
      "[I 2025-07-25 23:14:32,166] Trial 10 finished with value: 0.8025755091550476 and parameters: {'lambda_l1': 1.958669741747366e-06, 'lambda_l2': 1.4041959557429555e-08, 'num_leaves': 256, 'feature_fraction': 0.650839451622335, 'bagging_fraction': 0.742696929278615, 'bagging_freq': 3, 'min_child_samples': 84, 'min_gain_to_split': 0.7456617045186738, 'learning_rate': 0.003620641658156131}. Best is trial 1 with value: 0.8042699600916954.\n",
      "[I 2025-07-25 23:14:37,495] Trial 11 finished with value: 0.8016659993179897 and parameters: {'lambda_l1': 9.762940011102904e-05, 'lambda_l2': 9.908287192446451e-08, 'num_leaves': 8, 'feature_fraction': 0.670569327541006, 'bagging_fraction': 0.4055788316123092, 'bagging_freq': 1, 'min_child_samples': 47, 'min_gain_to_split': 3.1500455993487853, 'learning_rate': 0.008692610262642505}. Best is trial 1 with value: 0.8042699600916954.\n",
      "[I 2025-07-25 23:14:41,333] Trial 12 finished with value: 0.7554607928630789 and parameters: {'lambda_l1': 7.040104434278121e-06, 'lambda_l2': 3.333792780948274e-06, 'num_leaves': 2, 'feature_fraction': 0.7690245493653816, 'bagging_fraction': 0.679588967441187, 'bagging_freq': 1, 'min_child_samples': 70, 'min_gain_to_split': 3.532493707066838, 'learning_rate': 0.004825752174716126}. Best is trial 1 with value: 0.8042699600916954.\n",
      "[I 2025-07-25 23:14:47,804] Trial 13 finished with value: 0.7925714388890979 and parameters: {'lambda_l1': 8.825127922659911, 'lambda_l2': 7.286392772733657e-07, 'num_leaves': 59, 'feature_fraction': 0.5869898034053584, 'bagging_fraction': 0.5329567267726586, 'bagging_freq': 2, 'min_child_samples': 34, 'min_gain_to_split': 2.082318209126045, 'learning_rate': 0.0022820360206495683}. Best is trial 1 with value: 0.8042699600916954.\n",
      "[I 2025-07-25 23:14:58,496] Trial 14 finished with value: 0.8030288356069981 and parameters: {'lambda_l1': 4.572178949662456e-08, 'lambda_l2': 1.225370363819109e-08, 'num_leaves': 42, 'feature_fraction': 0.7637746139468031, 'bagging_fraction': 0.7807966875407321, 'bagging_freq': 2, 'min_child_samples': 56, 'min_gain_to_split': 1.1406533712275495, 'learning_rate': 0.013591987577882145}. Best is trial 1 with value: 0.8042699600916954.\n",
      "[I 2025-07-25 23:15:01,431] Trial 15 finished with value: 0.8013393768452806 and parameters: {'lambda_l1': 0.002485714661251253, 'lambda_l2': 4.0427560853163426e-06, 'num_leaves': 214, 'feature_fraction': 0.5880760180271821, 'bagging_fraction': 0.6861245423729473, 'bagging_freq': 3, 'min_child_samples': 100, 'min_gain_to_split': 2.8342473809051114, 'learning_rate': 0.10064880475702055}. Best is trial 1 with value: 0.8042699600916954.\n",
      "[I 2025-07-25 23:15:19,199] Trial 16 finished with value: 0.8023746800291824 and parameters: {'lambda_l1': 8.388183274439353e-06, 'lambda_l2': 1.1857860811590339e-05, 'num_leaves': 113, 'feature_fraction': 0.7207079686483946, 'bagging_fraction': 0.5905263401750301, 'bagging_freq': 1, 'min_child_samples': 74, 'min_gain_to_split': 3.6039671630359575, 'learning_rate': 0.006761755599204499}. Best is trial 1 with value: 0.8042699600916954.\n",
      "[I 2025-07-25 23:15:25,880] Trial 17 finished with value: 0.8040908760233023 and parameters: {'lambda_l1': 6.019792973403656e-07, 'lambda_l2': 1.3647556646326696e-07, 'num_leaves': 201, 'feature_fraction': 0.8437317503914065, 'bagging_fraction': 0.482531633011423, 'bagging_freq': 2, 'min_child_samples': 55, 'min_gain_to_split': 2.4162251483524004, 'learning_rate': 0.020814430566988083}. Best is trial 1 with value: 0.8042699600916954.\n",
      "[I 2025-07-25 23:15:31,939] Trial 18 finished with value: 0.8041685334363177 and parameters: {'lambda_l1': 1.8875003001498962e-06, 'lambda_l2': 2.43884595725497e-07, 'num_leaves': 251, 'feature_fraction': 0.8586850614147434, 'bagging_fraction': 0.8298504236356212, 'bagging_freq': 2, 'min_child_samples': 44, 'min_gain_to_split': 2.3300136313752082, 'learning_rate': 0.10557743415850235}. Best is trial 1 with value: 0.8042699600916954.\n",
      "[I 2025-07-25 23:15:34,610] Trial 19 finished with value: 0.8028234032932774 and parameters: {'lambda_l1': 4.8128438344895e-05, 'lambda_l2': 0.000852886830174099, 'num_leaves': 253, 'feature_fraction': 0.8762958778638748, 'bagging_fraction': 0.8349072903074254, 'bagging_freq': 4, 'min_child_samples': 26, 'min_gain_to_split': 1.1616410102714145, 'learning_rate': 0.11028810730295692}. Best is trial 1 with value: 0.8042699600916954.\n",
      "[I 2025-07-25 23:15:36,778] Trial 20 finished with value: 0.8044603066765657 and parameters: {'lambda_l1': 3.084538498029943e-07, 'lambda_l2': 1.2625882692195595e-05, 'num_leaves': 233, 'feature_fraction': 0.8368455989004636, 'bagging_fraction': 0.992951913472188, 'bagging_freq': 3, 'min_child_samples': 43, 'min_gain_to_split': 1.4504647576559353, 'learning_rate': 0.14394462558291002}. Best is trial 20 with value: 0.8044603066765657.\n",
      "[I 2025-07-25 23:15:39,060] Trial 21 finished with value: 0.8016548723968231 and parameters: {'lambda_l1': 3.2947909686290053e-07, 'lambda_l2': 2.3472698085912744e-05, 'num_leaves': 232, 'feature_fraction': 0.8347338720516219, 'bagging_fraction': 0.9893118478838678, 'bagging_freq': 3, 'min_child_samples': 44, 'min_gain_to_split': 1.39003510410955, 'learning_rate': 0.1251967479985051}. Best is trial 20 with value: 0.8044603066765657.\n",
      "[I 2025-07-25 23:15:43,716] Trial 22 finished with value: 0.8038232948297803 and parameters: {'lambda_l1': 1.0990376300427927e-08, 'lambda_l2': 5.395208083381248e-07, 'num_leaves': 190, 'feature_fraction': 0.596928478789451, 'bagging_fraction': 0.9242808850022862, 'bagging_freq': 2, 'min_child_samples': 32, 'min_gain_to_split': 2.2985112887064556, 'learning_rate': 0.16699470499628027}. Best is trial 20 with value: 0.8044603066765657.\n",
      "[I 2025-07-25 23:15:48,157] Trial 23 finished with value: 0.8027057103298063 and parameters: {'lambda_l1': 1.379737376502031e-05, 'lambda_l2': 1.3450342750014053e-05, 'num_leaves': 230, 'feature_fraction': 0.716575234617789, 'bagging_fraction': 0.8728829576149879, 'bagging_freq': 3, 'min_child_samples': 64, 'min_gain_to_split': 0.5824595230528673, 'learning_rate': 0.061413314965825735}. Best is trial 20 with value: 0.8044603066765657.\n",
      "[I 2025-07-25 23:15:50,437] Trial 24 finished with value: 0.8037447380348507 and parameters: {'lambda_l1': 5.248057040838386e-07, 'lambda_l2': 4.987089876451725e-07, 'num_leaves': 239, 'feature_fraction': 0.8127274779778781, 'bagging_fraction': 0.9691889452906924, 'bagging_freq': 2, 'min_child_samples': 47, 'min_gain_to_split': 1.5996397018304642, 'learning_rate': 0.17497977188259442}. Best is trial 20 with value: 0.8044603066765657.\n",
      "[I 2025-07-25 23:15:52,417] Trial 25 finished with value: 0.8030334040674914 and parameters: {'lambda_l1': 2.5040842640567258e-06, 'lambda_l2': 0.0026094875695786084, 'num_leaves': 191, 'feature_fraction': 0.8936373490654085, 'bagging_fraction': 0.7444056410424474, 'bagging_freq': 4, 'min_child_samples': 24, 'min_gain_to_split': 2.845831522914989, 'learning_rate': 0.07693434567452206}. Best is trial 20 with value: 0.8044603066765657.\n",
      "[I 2025-07-25 23:15:58,452] Trial 26 finished with value: 0.8025683412075428 and parameters: {'lambda_l1': 7.033657548960555e-08, 'lambda_l2': 7.534103346126368e-05, 'num_leaves': 242, 'feature_fraction': 0.9844700172472747, 'bagging_fraction': 0.8684813541008608, 'bagging_freq': 2, 'min_child_samples': 39, 'min_gain_to_split': 2.3252514520610994, 'learning_rate': 0.03458295697480872}. Best is trial 20 with value: 0.8044603066765657.\n",
      "[I 2025-07-25 23:16:03,009] Trial 27 finished with value: 0.8022072521526102 and parameters: {'lambda_l1': 0.000712577701484717, 'lambda_l2': 3.6874643797647517e-06, 'num_leaves': 214, 'feature_fraction': 0.8613559893991696, 'bagging_fraction': 0.9469113809193263, 'bagging_freq': 5, 'min_child_samples': 81, 'min_gain_to_split': 0.8326976892644851, 'learning_rate': 0.03832394736818653}. Best is trial 20 with value: 0.8044603066765657.\n",
      "[I 2025-07-25 23:16:04,862] Trial 28 finished with value: 0.8018982065738365 and parameters: {'lambda_l1': 3.2832313734533084e-05, 'lambda_l2': 2.0896656457221247e-07, 'num_leaves': 255, 'feature_fraction': 0.7357516350582775, 'bagging_fraction': 0.8166346360334656, 'bagging_freq': 1, 'min_child_samples': 18, 'min_gain_to_split': 1.6768367054577666, 'learning_rate': 0.16764779588856432}. Best is trial 20 with value: 0.8044603066765657.\n",
      "[I 2025-07-25 23:16:16,619] Trial 29 finished with value: 0.8038174264487898 and parameters: {'lambda_l1': 1.8675673506501023e-06, 'lambda_l2': 4.899070075780382e-08, 'num_leaves': 223, 'feature_fraction': 0.9624907767296911, 'bagging_fraction': 0.9976254763305821, 'bagging_freq': 3, 'min_child_samples': 39, 'min_gain_to_split': 0.18460235025069416, 'learning_rate': 0.07247010576528226}. Best is trial 20 with value: 0.8044603066765657.\n",
      "[I 2025-07-25 23:16:37,606] Trial 30 finished with value: 0.804079157751055 and parameters: {'lambda_l1': 2.755724159751255e-07, 'lambda_l2': 0.42244876672696885, 'num_leaves': 137, 'feature_fraction': 0.9409248638559136, 'bagging_fraction': 0.8655827533007975, 'bagging_freq': 1, 'min_child_samples': 45, 'min_gain_to_split': 2.6243746099334393, 'learning_rate': 0.011697087743907732}. Best is trial 20 with value: 0.8044603066765657.\n",
      "[I 2025-07-25 23:16:29,289] Trial 31 finished with value: 0.80486272869865 and parameters: {'lambda_l1': 3.8703083534527313e-07, 'lambda_l2': 1.404859564166942e-06, 'num_leaves': 198, 'feature_fraction': 0.8265831480976773, 'bagging_fraction': 0.4311257530887381, 'bagging_freq': 2, 'min_child_samples': 55, 'min_gain_to_split': 2.1998221633826445, 'learning_rate': 0.02380717597262117}. Best is trial 31 with value: 0.80486272869865.\n",
      "[I 2025-07-25 23:16:33,523] Trial 32 finished with value: 0.8032539271933148 and parameters: {'lambda_l1': 1.0092781704753468e-08, 'lambda_l2': 2.0837644882207345e-06, 'num_leaves': 197, 'feature_fraction': 0.895533761081055, 'bagging_fraction': 0.4130636321161268, 'bagging_freq': 2, 'min_child_samples': 60, 'min_gain_to_split': 2.12393986143948, 'learning_rate': 0.04726466091079935}. Best is trial 31 with value: 0.80486272869865.\n",
      "[I 2025-07-25 23:16:45,715] Trial 33 finished with value: 0.8031551539064606 and parameters: {'lambda_l1': 1.0200440250859128e-07, 'lambda_l2': 2.590090893228014e-05, 'num_leaves': 179, 'feature_fraction': 0.8120183136560283, 'bagging_fraction': 0.6535677761401235, 'bagging_freq': 3, 'min_child_samples': 66, 'min_gain_to_split': 3.0527023404770137, 'learning_rate': 0.006166675095460289}. Best is trial 31 with value: 0.80486272869865.\n",
      "[I 2025-07-25 23:16:54,553] Trial 34 finished with value: 0.8054454912355808 and parameters: {'lambda_l1': 4.184938507731707e-06, 'lambda_l2': 7.547150903009415e-06, 'num_leaves': 207, 'feature_fraction': 0.617486120136056, 'bagging_fraction': 0.7340908522797027, 'bagging_freq': 1, 'min_child_samples': 49, 'min_gain_to_split': 1.4319343813942214, 'learning_rate': 0.016627745598180605}. Best is trial 34 with value: 0.8054454912355808.\n",
      "[I 2025-07-25 23:17:01,836] Trial 35 finished with value: 0.8047643904955141 and parameters: {'lambda_l1': 0.00020445172215319174, 'lambda_l2': 0.0003291602092530277, 'num_leaves': 207, 'feature_fraction': 0.5659211001875232, 'bagging_fraction': 0.7385166245822892, 'bagging_freq': 1, 'min_child_samples': 51, 'min_gain_to_split': 1.363658850812999, 'learning_rate': 0.015643823626824017}. Best is trial 34 with value: 0.8054454912355808.\n",
      "[I 2025-07-25 23:17:07,904] Trial 36 finished with value: 0.8033720911684933 and parameters: {'lambda_l1': 0.00022633475316392152, 'lambda_l2': 0.00018386133885639754, 'num_leaves': 211, 'feature_fraction': 0.5212157275454703, 'bagging_fraction': 0.7227132564182333, 'bagging_freq': 1, 'min_child_samples': 51, 'min_gain_to_split': 1.3643562803441576, 'learning_rate': 0.01695729284090981}. Best is trial 34 with value: 0.8054454912355808.\n",
      "[I 2025-07-25 23:17:13,476] Trial 37 finished with value: 0.8018757991070211 and parameters: {'lambda_l1': 0.005782449619102217, 'lambda_l2': 0.0018192787886663574, 'num_leaves': 179, 'feature_fraction': 0.5528376895853667, 'bagging_fraction': 0.6333477791963953, 'bagging_freq': 1, 'min_child_samples': 51, 'min_gain_to_split': 1.046482084728646, 'learning_rate': 0.02524719021966795}. Best is trial 34 with value: 0.8054454912355808.\n",
      "[I 2025-07-25 23:17:22,558] Trial 38 finished with value: 0.8047537015321007 and parameters: {'lambda_l1': 0.0008752099480015938, 'lambda_l2': 4.664796321024292e-05, 'num_leaves': 121, 'feature_fraction': 0.6232710986132808, 'bagging_fraction': 0.5395390304699401, 'bagging_freq': 1, 'min_child_samples': 32, 'min_gain_to_split': 0.37690033185961336, 'learning_rate': 0.016403878781606367}. Best is trial 34 with value: 0.8054454912355808.\n",
      "[I 2025-07-25 23:17:33,803] Trial 39 finished with value: 0.8028967262255373 and parameters: {'lambda_l1': 0.0011058219696992255, 'lambda_l2': 0.0005424255324331239, 'num_leaves': 148, 'feature_fraction': 0.6178064738668455, 'bagging_fraction': 0.44041736470176496, 'bagging_freq': 1, 'min_child_samples': 29, 'min_gain_to_split': 0.3762595491712633, 'learning_rate': 0.017182688315951707}. Best is trial 34 with value: 0.8054454912355808.\n",
      "[I 2025-07-25 23:17:41,649] Trial 40 finished with value: 0.804803971507076 and parameters: {'lambda_l1': 0.016592606735420085, 'lambda_l2': 0.016215812653060636, 'num_leaves': 125, 'feature_fraction': 0.41888358446607044, 'bagging_fraction': 0.5234130887791519, 'bagging_freq': 1, 'min_child_samples': 18, 'min_gain_to_split': 1.851481583008657, 'learning_rate': 0.008716876359426378}. Best is trial 34 with value: 0.8054454912355808.\n",
      "[I 2025-07-25 23:17:47,886] Trial 41 finished with value: 0.803262620772933 and parameters: {'lambda_l1': 0.010860110727535937, 'lambda_l2': 0.040899791310519296, 'num_leaves': 118, 'feature_fraction': 0.4845819419270475, 'bagging_fraction': 0.5217303395732329, 'bagging_freq': 1, 'min_child_samples': 22, 'min_gain_to_split': 1.847689550158784, 'learning_rate': 0.015070465792050378}. Best is trial 34 with value: 0.8054454912355808.\n",
      "[I 2025-07-25 23:18:06,162] Trial 42 finished with value: 0.8055313958940129 and parameters: {'lambda_l1': 0.02401760408646057, 'lambda_l2': 0.008131454157467705, 'num_leaves': 92, 'feature_fraction': 0.5571261663518792, 'bagging_fraction': 0.5735762581660586, 'bagging_freq': 1, 'min_child_samples': 12, 'min_gain_to_split': 0.9020076179879019, 'learning_rate': 0.008074542324563254}. Best is trial 42 with value: 0.8055313958940129.\n",
      "[I 2025-07-25 23:18:23,337] Trial 43 finished with value: 0.8024826955563074 and parameters: {'lambda_l1': 0.33886637980745704, 'lambda_l2': 0.014703948161705562, 'num_leaves': 94, 'feature_fraction': 0.4155988093544018, 'bagging_fraction': 0.5852865556878397, 'bagging_freq': 1, 'min_child_samples': 15, 'min_gain_to_split': 0.9253070303174418, 'learning_rate': 0.00935803797283883}. Best is trial 42 with value: 0.8055313958940129.\n",
      "[I 2025-07-25 23:18:40,207] Trial 44 finished with value: 0.8043468638313935 and parameters: {'lambda_l1': 0.02171818594052469, 'lambda_l2': 0.006038376082101685, 'num_leaves': 72, 'feature_fraction': 0.5567327779418333, 'bagging_fraction': 0.45595632451958057, 'bagging_freq': 7, 'min_child_samples': 8, 'min_gain_to_split': 1.96958113064297, 'learning_rate': 0.0026255606728107232}. Best is trial 42 with value: 0.8055313958940129.\n",
      "[I 2025-07-25 23:18:48,571] Trial 45 finished with value: 0.8033852120312016 and parameters: {'lambda_l1': 0.6089306395539549, 'lambda_l2': 0.07962911345716883, 'num_leaves': 137, 'feature_fraction': 0.4442022646895576, 'bagging_fraction': 0.49123244705164787, 'bagging_freq': 6, 'min_child_samples': 6, 'min_gain_to_split': 1.6159091112255561, 'learning_rate': 0.007289375215799424}. Best is trial 42 with value: 0.8055313958940129.\n",
      "[I 2025-07-25 23:19:10,410] Trial 46 finished with value: 0.8038615276051754 and parameters: {'lambda_l1': 0.004203059378749126, 'lambda_l2': 0.17562690797484642, 'num_leaves': 104, 'feature_fraction': 0.6822460757754245, 'bagging_fraction': 0.7833594762535979, 'bagging_freq': 1, 'min_child_samples': 19, 'min_gain_to_split': 1.2052248010492288, 'learning_rate': 0.0043463825007420106}. Best is trial 42 with value: 0.8055313958940129.\n",
      "[I 2025-07-25 23:19:19,536] Trial 47 finished with value: 0.8030318910052859 and parameters: {'lambda_l1': 9.463895076512642e-05, 'lambda_l2': 2.060061602248951, 'num_leaves': 164, 'feature_fraction': 0.5481212673307757, 'bagging_fraction': 0.5697547098609014, 'bagging_freq': 2, 'min_child_samples': 12, 'min_gain_to_split': 0.5563414579803606, 'learning_rate': 0.024819563576712566}. Best is trial 42 with value: 0.8055313958940129.\n",
      "[I 2025-07-25 23:19:25,048] Trial 48 finished with value: 0.7984548551333489 and parameters: {'lambda_l1': 8.552498611887, 'lambda_l2': 0.014857681577358148, 'num_leaves': 80, 'feature_fraction': 0.47521509863439104, 'bagging_fraction': 0.7136749259360943, 'bagging_freq': 1, 'min_child_samples': 57, 'min_gain_to_split': 2.0954870483355794, 'learning_rate': 0.010335702429307961}. Best is trial 42 with value: 0.8055313958940129.\n",
      "[I 2025-07-25 23:19:34,596] Trial 49 finished with value: 0.8033872655493676 and parameters: {'lambda_l1': 0.11129882894109026, 'lambda_l2': 0.004363184027783795, 'num_leaves': 129, 'feature_fraction': 0.40462048279918594, 'bagging_fraction': 0.6599319331117176, 'bagging_freq': 1, 'min_child_samples': 63, 'min_gain_to_split': 1.4417129214115427, 'learning_rate': 0.00588083136785546}. Best is trial 42 with value: 0.8055313958940129.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Optuna チューニング結果 ---\n",
      "最適なパラメータ: {'lambda_l1': 0.02401760408646057, 'lambda_l2': 0.008131454157467705, 'num_leaves': 92, 'feature_fraction': 0.5571261663518792, 'bagging_fraction': 0.5735762581660586, 'bagging_freq': 1, 'min_child_samples': 12, 'min_gain_to_split': 0.9020076179879019, 'learning_rate': 0.008074542324563254}\n",
      "最適なCVスコア (Accuracy): 0.8055313958940129\n",
      "\n",
      "--- モデル学習完了 ---\n",
      "\n",
      "--- テストデータ予測完了 ---\n",
      "予測された Transported の分布:\n",
      "True     2215\n",
      "False    2062\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: '../outputs/submissions/ver5'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 132\u001b[39m\n\u001b[32m    130\u001b[39m \u001b[38;5;66;03m# 提出ファイルを保存\u001b[39;00m\n\u001b[32m    131\u001b[39m submission_path = \u001b[33m'\u001b[39m\u001b[33m../outputs/submissions/ver5/soya_model5.csv\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m \u001b[43msubmission\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubmission_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m提出ファイルを保存しました: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubmission_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    135\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m--- セル3完了 ---\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/util/_decorators.py:333\u001b[39m, in \u001b[36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) > num_allow_args:\n\u001b[32m    328\u001b[39m     warnings.warn(\n\u001b[32m    329\u001b[39m         msg.format(arguments=_format_argument_list(allow_args)),\n\u001b[32m    330\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m    331\u001b[39m         stacklevel=find_stack_level(),\n\u001b[32m    332\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m333\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/core/generic.py:3986\u001b[39m, in \u001b[36mNDFrame.to_csv\u001b[39m\u001b[34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[39m\n\u001b[32m   3975\u001b[39m df = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ABCDataFrame) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.to_frame()\n\u001b[32m   3977\u001b[39m formatter = DataFrameFormatter(\n\u001b[32m   3978\u001b[39m     frame=df,\n\u001b[32m   3979\u001b[39m     header=header,\n\u001b[32m   (...)\u001b[39m\u001b[32m   3983\u001b[39m     decimal=decimal,\n\u001b[32m   3984\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m3986\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mDataFrameRenderer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mformatter\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3987\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3988\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlineterminator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3989\u001b[39m \u001b[43m    \u001b[49m\u001b[43msep\u001b[49m\u001b[43m=\u001b[49m\u001b[43msep\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3990\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3991\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3992\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3993\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquoting\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3994\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3995\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex_label\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3996\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3997\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3998\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquotechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3999\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdate_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4000\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdoublequote\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4001\u001b[39m \u001b[43m    \u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mescapechar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4002\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   4003\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/formats/format.py:1014\u001b[39m, in \u001b[36mDataFrameRenderer.to_csv\u001b[39m\u001b[34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, lineterminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[39m\n\u001b[32m    993\u001b[39m     created_buffer = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    995\u001b[39m csv_formatter = CSVFormatter(\n\u001b[32m    996\u001b[39m     path_or_buf=path_or_buf,\n\u001b[32m    997\u001b[39m     lineterminator=lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1012\u001b[39m     formatter=\u001b[38;5;28mself\u001b[39m.fmt,\n\u001b[32m   1013\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m \u001b[43mcsv_formatter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m created_buffer:\n\u001b[32m   1017\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/formats/csvs.py:251\u001b[39m, in \u001b[36mCSVFormatter.save\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03mCreate the writer & save.\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    256\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    257\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    258\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m handles:\n\u001b[32m    259\u001b[39m     \u001b[38;5;66;03m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[32m    260\u001b[39m     \u001b[38;5;28mself\u001b[39m.writer = csvlib.writer(\n\u001b[32m    261\u001b[39m         handles.handle,\n\u001b[32m    262\u001b[39m         lineterminator=\u001b[38;5;28mself\u001b[39m.lineterminator,\n\u001b[32m   (...)\u001b[39m\u001b[32m    267\u001b[39m         quotechar=\u001b[38;5;28mself\u001b[39m.quotechar,\n\u001b[32m    268\u001b[39m     )\n\u001b[32m    270\u001b[39m     \u001b[38;5;28mself\u001b[39m._save()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/common.py:749\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    747\u001b[39m \u001b[38;5;66;03m# Only for write methods\u001b[39;00m\n\u001b[32m    748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m is_path:\n\u001b[32m--> \u001b[39m\u001b[32m749\u001b[39m     \u001b[43mcheck_parent_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m compression:\n\u001b[32m    752\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m compression != \u001b[33m\"\u001b[39m\u001b[33mzstd\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    753\u001b[39m         \u001b[38;5;66;03m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/miniforge3/lib/python3.12/site-packages/pandas/io/common.py:616\u001b[39m, in \u001b[36mcheck_parent_directory\u001b[39m\u001b[34m(path)\u001b[39m\n\u001b[32m    614\u001b[39m parent = Path(path).parent\n\u001b[32m    615\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m parent.is_dir():\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33mrf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCannot save file into a non-existent directory: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparent\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mOSError\u001b[39m: Cannot save file into a non-existent directory: '../outputs/submissions/ver5'"
     ]
    }
   ],
   "source": [
    "# セル 3: モデルの選定・学習・評価 (LightGBM + Optuna + StratifiedGroupKFold)\n",
    "\n",
    "# --- 1. モデルとCVの設定 ---\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
    "\n",
    "# GroupID を整数に変換 (StratifiedGroupKFold 用)\n",
    "# X_train_processed に GroupID がないので、元の X_train_base から取得\n",
    "groups = X_train_base['GroupID'].reset_index(drop=True) # インデックスをリセットしてから取得\n",
    "groups_int = pd.to_numeric(groups, errors='coerce').fillna(0).astype(int)\n",
    "\n",
    "# StratifiedGroupKFold の設定\n",
    "# 知識ベースによると CV vs LB ギャップがあるので、SGKFold を使用\n",
    "sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# --- 2. Optunaによるハイパーパラメーターチューニング ---\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    Optuna の目的関数。LightGBM のパラメータを提案し、CVスコアを返す。\n",
    "    \"\"\"\n",
    "    param = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": \"binary_logloss\", # Optuna の maximize/minimize は accuracy には向いていないため、logloss を最小化する\n",
    "        \"verbosity\": -1,\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        # \"device_type\": \"gpu\", # GPU を使用する場合はコメントアウトを外す (環境依存)\n",
    "        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n",
    "        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n",
    "        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n",
    "        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n",
    "        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n",
    "        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n",
    "        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n",
    "        \"min_gain_to_split\": trial.suggest_float(\"min_gain_to_split\", 0, 5),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.001, 0.3, log=True), # logスケールで探索\n",
    "    }\n",
    "\n",
    "    # CVスコアを格納するリスト\n",
    "    cv_scores = []\n",
    "\n",
    "    # StratifiedGroupKFold で分割\n",
    "    for fold, (train_idx, val_idx) in enumerate(sgkf.split(X_train_processed, y_train, groups=groups_int)):\n",
    "        # データ分割\n",
    "        X_train_fold = X_train_processed.iloc[train_idx]\n",
    "        y_train_fold = y_train.iloc[train_idx]\n",
    "        X_val_fold = X_train_processed.iloc[val_idx]\n",
    "        y_val_fold = y_train.iloc[val_idx]\n",
    "\n",
    "        # LightGBM Dataset の作成\n",
    "        train_data = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "        val_data = lgb.Dataset(X_val_fold, label=y_val_fold, reference=train_data)\n",
    "\n",
    "        # モデル学習\n",
    "        model = lgb.train(\n",
    "            param,\n",
    "            train_data,\n",
    "            valid_sets=[val_data],\n",
    "            valid_names=['valid'],\n",
    "            num_boost_round=1000,\n",
    "            callbacks=[\n",
    "                lgb.early_stopping(stopping_rounds=100, verbose=False),\n",
    "                lgb.log_evaluation(period=0) # 学習ログを非表示\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # 予測 (確率)\n",
    "        y_pred_proba = model.predict(X_val_fold, num_iteration=model.best_iteration)\n",
    "        # 確率をラベルに変換 (0.5 以上を True)\n",
    "        y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "        # Accuracy を計算\n",
    "        acc = accuracy_score(y_val_fold, y_pred)\n",
    "        cv_scores.append(acc)\n",
    "\n",
    "    # 平均CVスコアを返す (Optuna はデフォルトで minimize するので、-1をかける)\n",
    "    mean_cv_score = np.mean(cv_scores)\n",
    "    return mean_cv_score # Accuracy を直接最大化するため、マイナスはつけない\n",
    "\n",
    "# Optuna のスタディを作成\n",
    "study = optuna.create_study(direction='maximize') # Accuracy を最大化\n",
    "print(\"Optuna チューニングを開始します...\")\n",
    "# 試行回数を制限 (例: 50回) して実行時間をコントロール\n",
    "study.optimize(objective, n_trials=50) # 実際には100~200程度が望ましいが、ここでは50回で様子見\n",
    "\n",
    "print(\"\\n--- Optuna チューニング結果 ---\")\n",
    "print(\"最適なパラメータ:\", study.best_params)\n",
    "print(\"最適なCVスコア (Accuracy):\", study.best_value)\n",
    "\n",
    "# --- 3. 最適パラメータでモデルを学習 ---\n",
    "best_params = study.best_params\n",
    "best_params[\"objective\"] = \"binary\"\n",
    "best_params[\"metric\"] = \"binary_logloss\"\n",
    "best_params[\"verbosity\"] = -1\n",
    "# best_params[\"device_type\"] = \"gpu\" # GPU 使用時\n",
    "\n",
    "# 全データで学習 (CVの設定は不要)\n",
    "final_train_data = lgb.Dataset(X_train_processed, label=y_train)\n",
    "# --- 修正: callbacks から early_stopping を削除 ---\n",
    "final_model = lgb.train(\n",
    "    best_params,\n",
    "    final_train_data,\n",
    "    num_boost_round=1000, # 固定のiteration数、または study.best_trial から取得した平均 best_iteration を使う\n",
    "    callbacks=[\n",
    "        # lgb.early_stopping(stopping_rounds=100, verbose=True), # 削除\n",
    "        lgb.log_evaluation(period=100)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"\\n--- モデル学習完了 ---\")\n",
    "\n",
    "# --- 4. テストデータへの予測 ---\n",
    "# 確率で予測\n",
    "y_test_pred_proba = final_model.predict(X_test_processed, num_iteration=final_model.best_iteration)\n",
    "# 0.5 を閾値としてラベルに変換\n",
    "y_test_pred = (y_test_pred_proba >= 0.5).astype(bool)\n",
    "\n",
    "print(\"\\n--- テストデータ予測完了 ---\")\n",
    "print(f\"予測された Transported の分布:\")\n",
    "print(pd.Series(y_test_pred).value_counts())\n",
    "\n",
    "# --- 5. 提出ファイルの作成 ---\n",
    "submission = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Transported': y_test_pred\n",
    "})\n",
    "\n",
    "# 提出ファイルを保存\n",
    "submission_path = '../outputs/submissions/ver5/soya_model5.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f\"\\n提出ファイルを保存しました: {submission_path}\")\n",
    "\n",
    "print(\"\\n--- セル3完了 ---\")\n",
    "print(\"次のステップ: 結果の確認と提出\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6094fea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 23:32:56,482] A new study created in memory with name: no-name-81742b2d-b865-4dc8-aa25-96370e9241cf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna チューニングを開始します (XGBoost)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 23:33:00,314] Trial 0 finished with value: 0.7877112755031714 and parameters: {'lambda': 0.1592104435040895, 'alpha': 1.2299473567219701e-06, 'colsample_bytree': 0.4152656145740945, 'subsample': 0.9731882244224208, 'learning_rate': 0.008916214523288413, 'n_estimators': 538, 'max_depth': 7, 'min_child_weight': 147}. Best is trial 0 with value: 0.7877112755031714.\n",
      "[I 2025-07-25 23:33:03,236] Trial 1 finished with value: 0.7952116363103086 and parameters: {'lambda': 1.4077453553473529e-05, 'alpha': 3.9967678426707544e-07, 'colsample_bytree': 0.6153785590682018, 'subsample': 0.9796432590908821, 'learning_rate': 0.1960994366731847, 'n_estimators': 560, 'max_depth': 6, 'min_child_weight': 18}. Best is trial 1 with value: 0.7952116363103086.\n",
      "[I 2025-07-25 23:33:05,970] Trial 2 finished with value: 0.7938282633582667 and parameters: {'lambda': 0.00012559841033867112, 'alpha': 4.056726595749755e-08, 'colsample_bytree': 0.5745175962601097, 'subsample': 0.6661925898431725, 'learning_rate': 0.07749680780516438, 'n_estimators': 777, 'max_depth': 3, 'min_child_weight': 47}. Best is trial 1 with value: 0.7952116363103086.\n",
      "[I 2025-07-25 23:33:09,358] Trial 3 finished with value: 0.7900196285900222 and parameters: {'lambda': 0.00044216095925365647, 'alpha': 1.2705767126137063e-06, 'colsample_bytree': 0.4697337458439288, 'subsample': 0.5084915666069223, 'learning_rate': 0.03563191657985722, 'n_estimators': 788, 'max_depth': 10, 'min_child_weight': 61}. Best is trial 1 with value: 0.7952116363103086.\n",
      "[I 2025-07-25 23:33:11,434] Trial 4 finished with value: 0.7449560715532786 and parameters: {'lambda': 0.4829645710631977, 'alpha': 0.15593989780452866, 'colsample_bytree': 0.9229314736529199, 'subsample': 0.6034498000400199, 'learning_rate': 0.056869003501762494, 'n_estimators': 451, 'max_depth': 9, 'min_child_weight': 278}. Best is trial 1 with value: 0.7952116363103086.\n",
      "[I 2025-07-25 23:33:17,297] Trial 5 finished with value: 0.7977634463472991 and parameters: {'lambda': 0.0006500984136076754, 'alpha': 7.907671256650476e-05, 'colsample_bytree': 0.8864784966890211, 'subsample': 0.5029688597739377, 'learning_rate': 0.01484797099052941, 'n_estimators': 841, 'max_depth': 9, 'min_child_weight': 48}. Best is trial 5 with value: 0.7977634463472991.\n",
      "[I 2025-07-25 23:33:19,841] Trial 6 finished with value: 0.7640881218986987 and parameters: {'lambda': 0.00255646181254989, 'alpha': 3.745928280625589e-07, 'colsample_bytree': 0.7540934408441047, 'subsample': 0.90927795310899, 'learning_rate': 0.006213985143045065, 'n_estimators': 571, 'max_depth': 11, 'min_child_weight': 245}. Best is trial 5 with value: 0.7977634463472991.\n",
      "[I 2025-07-25 23:33:21,469] Trial 7 finished with value: 0.7686896524844582 and parameters: {'lambda': 3.2801087265601635e-08, 'alpha': 1.56389290625641e-05, 'colsample_bytree': 0.8912052697330404, 'subsample': 0.497148528657095, 'learning_rate': 0.005021798116417387, 'n_estimators': 109, 'max_depth': 12, 'min_child_weight': 95}. Best is trial 5 with value: 0.7977634463472991.\n",
      "[I 2025-07-25 23:33:24,777] Trial 8 finished with value: 0.7714019368755214 and parameters: {'lambda': 3.5583019777023694, 'alpha': 3.892327443768864e-05, 'colsample_bytree': 0.6974135080590016, 'subsample': 0.4246513180299024, 'learning_rate': 0.06522775316210715, 'n_estimators': 825, 'max_depth': 11, 'min_child_weight': 124}. Best is trial 5 with value: 0.7977634463472991.\n",
      "[I 2025-07-25 23:33:28,769] Trial 9 finished with value: 0.7935154666925308 and parameters: {'lambda': 0.00018389331218550701, 'alpha': 0.15546375566995194, 'colsample_bytree': 0.5292467409234478, 'subsample': 0.9407596316115717, 'learning_rate': 0.02350262052793765, 'n_estimators': 594, 'max_depth': 9, 'min_child_weight': 96}. Best is trial 5 with value: 0.7977634463472991.\n",
      "[I 2025-07-25 23:33:33,115] Trial 10 finished with value: 0.7574407351172322 and parameters: {'lambda': 4.993677323168945e-07, 'alpha': 0.004395618900352084, 'colsample_bytree': 0.9961424429195298, 'subsample': 0.7911061534485794, 'learning_rate': 0.0012685216126532644, 'n_estimators': 999, 'max_depth': 5, 'min_child_weight': 211}. Best is trial 5 with value: 0.7977634463472991.\n",
      "[I 2025-07-25 23:33:36,125] Trial 11 finished with value: 0.7933974361897443 and parameters: {'lambda': 2.7978119933238985e-06, 'alpha': 0.001205596524860257, 'colsample_bytree': 0.7519529837850122, 'subsample': 0.7826487877840954, 'learning_rate': 0.1664705533098495, 'n_estimators': 321, 'max_depth': 7, 'min_child_weight': 3}. Best is trial 5 with value: 0.7977634463472991.\n",
      "[I 2025-07-25 23:33:41,679] Trial 12 finished with value: 0.7841142717531523 and parameters: {'lambda': 6.319706211843652e-06, 'alpha': 1.2040847322677932e-08, 'colsample_bytree': 0.6477688852296892, 'subsample': 0.8120807345392476, 'learning_rate': 0.2644027050610134, 'n_estimators': 891, 'max_depth': 5, 'min_child_weight': 1}. Best is trial 5 with value: 0.7977634463472991.\n",
      "[I 2025-07-25 23:33:45,931] Trial 13 finished with value: 0.8005425560303493 and parameters: {'lambda': 0.0024737611824625807, 'alpha': 6.623245629745566e-05, 'colsample_bytree': 0.8328920127785523, 'subsample': 0.5985535506935774, 'learning_rate': 0.013304526651425598, 'n_estimators': 712, 'max_depth': 5, 'min_child_weight': 36}. Best is trial 13 with value: 0.8005425560303493.\n",
      "[I 2025-07-25 23:33:48,895] Trial 14 finished with value: 0.7660419156326813 and parameters: {'lambda': 0.010693623714370592, 'alpha': 0.00013285885708232655, 'colsample_bytree': 0.8401922252579089, 'subsample': 0.586568512289205, 'learning_rate': 0.012994284137781049, 'n_estimators': 706, 'max_depth': 3, 'min_child_weight': 186}. Best is trial 13 with value: 0.8005425560303493.\n",
      "[I 2025-07-25 23:33:54,811] Trial 15 finished with value: 0.789746298098888 and parameters: {'lambda': 0.02016800138378908, 'alpha': 0.007098176042180492, 'colsample_bytree': 0.8289692904288541, 'subsample': 0.40467460578194164, 'learning_rate': 0.0033859471283424272, 'n_estimators': 956, 'max_depth': 8, 'min_child_weight': 54}. Best is trial 13 with value: 0.8005425560303493.\n",
      "[I 2025-07-25 23:33:58,985] Trial 16 finished with value: 0.7852135219686455 and parameters: {'lambda': 0.0019083830880934228, 'alpha': 2.520280253885851, 'colsample_bytree': 0.995631157120688, 'subsample': 0.595368172910592, 'learning_rate': 0.0022868779923006346, 'n_estimators': 687, 'max_depth': 5, 'min_child_weight': 88}. Best is trial 13 with value: 0.8005425560303493.\n",
      "[I 2025-07-25 23:34:01,466] Trial 17 finished with value: 0.7952092621645016 and parameters: {'lambda': 0.05434999834079473, 'alpha': 5.781644018331019e-06, 'colsample_bytree': 0.8132199996082129, 'subsample': 0.49402140616190054, 'learning_rate': 0.01858694989691449, 'n_estimators': 407, 'max_depth': 4, 'min_child_weight': 38}. Best is trial 13 with value: 0.8005425560303493.\n",
      "[I 2025-07-25 23:34:04,638] Trial 18 finished with value: 0.7869976450763214 and parameters: {'lambda': 4.24711041223037e-05, 'alpha': 0.00033340901386373596, 'colsample_bytree': 0.9109469620843494, 'subsample': 0.6877886563166326, 'learning_rate': 0.029635763617224142, 'n_estimators': 685, 'max_depth': 8, 'min_child_weight': 132}. Best is trial 13 with value: 0.8005425560303493.\n",
      "[I 2025-07-25 23:34:07,873] Trial 19 finished with value: 0.7633144946932965 and parameters: {'lambda': 0.002242954052653482, 'alpha': 0.01952746991096688, 'colsample_bytree': 0.7347501848150599, 'subsample': 0.5472883543478068, 'learning_rate': 0.010916032932569267, 'n_estimators': 889, 'max_depth': 6, 'min_child_weight': 182}. Best is trial 13 with value: 0.8005425560303493.\n",
      "[I 2025-07-25 23:34:10,052] Trial 20 finished with value: 0.7764021625976774 and parameters: {'lambda': 3.7939452441839596, 'alpha': 9.72349213360975e-05, 'colsample_bytree': 0.8675943858481013, 'subsample': 0.6436099338887651, 'learning_rate': 0.0010127567639163742, 'n_estimators': 178, 'max_depth': 9, 'min_child_weight': 71}. Best is trial 13 with value: 0.8005425560303493.\n",
      "[I 2025-07-25 23:34:13,415] Trial 21 finished with value: 0.8008504336860867 and parameters: {'lambda': 1.625699447556121e-05, 'alpha': 2.1833639504685022e-07, 'colsample_bytree': 0.6215153035741252, 'subsample': 0.7148018823946223, 'learning_rate': 0.104223210321955, 'n_estimators': 504, 'max_depth': 6, 'min_child_weight': 19}. Best is trial 21 with value: 0.8008504336860867.\n",
      "[I 2025-07-25 23:34:16,437] Trial 22 finished with value: 0.7984645075486771 and parameters: {'lambda': 5.867771016810019e-07, 'alpha': 4.168588886687896e-06, 'colsample_bytree': 0.6650671184188444, 'subsample': 0.7391627637443391, 'learning_rate': 0.11499678574508955, 'n_estimators': 444, 'max_depth': 6, 'min_child_weight': 29}. Best is trial 21 with value: 0.8008504336860867.\n",
      "[I 2025-07-25 23:34:18,724] Trial 23 finished with value: 0.7998076785264654 and parameters: {'lambda': 8.769478187134867e-08, 'alpha': 7.502205161649257e-08, 'colsample_bytree': 0.6623517020340849, 'subsample': 0.7364337503417063, 'learning_rate': 0.11395376353208131, 'n_estimators': 291, 'max_depth': 6, 'min_child_weight': 25}. Best is trial 21 with value: 0.8008504336860867.\n",
      "[I 2025-07-25 23:34:20,566] Trial 24 finished with value: 0.802688759745469 and parameters: {'lambda': 2.1951709666388663e-08, 'alpha': 8.547169971217096e-08, 'colsample_bytree': 0.5521475730416037, 'subsample': 0.8615268396596035, 'learning_rate': 0.11366011585971071, 'n_estimators': 272, 'max_depth': 4, 'min_child_weight': 23}. Best is trial 24 with value: 0.802688759745469.\n",
      "[I 2025-07-25 23:34:22,348] Trial 25 finished with value: 0.7916848617782779 and parameters: {'lambda': 1.9263877287234706e-08, 'alpha': 1.4968676761252424e-07, 'colsample_bytree': 0.5464578364405528, 'subsample': 0.8547169719011667, 'learning_rate': 0.034115034567171354, 'n_estimators': 253, 'max_depth': 4, 'min_child_weight': 78}. Best is trial 24 with value: 0.802688759745469.\n",
      "[I 2025-07-25 23:34:24,497] Trial 26 finished with value: 0.7905582610506742 and parameters: {'lambda': 4.5685253651685326e-07, 'alpha': 3.9073041778643704e-08, 'colsample_bytree': 0.5877970414682915, 'subsample': 0.7280845296794435, 'learning_rate': 0.04837187237359436, 'n_estimators': 360, 'max_depth': 4, 'min_child_weight': 115}. Best is trial 24 with value: 0.802688759745469.\n",
      "[I 2025-07-25 23:34:27,465] Trial 27 finished with value: 0.8002755638338348 and parameters: {'lambda': 4.1076937130693024e-05, 'alpha': 1.8698494721782767e-06, 'colsample_bytree': 0.490656640146525, 'subsample': 0.8503150966511188, 'learning_rate': 0.07628578629471808, 'n_estimators': 499, 'max_depth': 5, 'min_child_weight': 20}. Best is trial 24 with value: 0.802688759745469.\n",
      "[I 2025-07-25 23:34:30,222] Trial 28 finished with value: 0.8018579407643216 and parameters: {'lambda': 1.8724355128143757e-06, 'alpha': 1.1208341652037772e-08, 'colsample_bytree': 0.7838730279234261, 'subsample': 0.8668462222553422, 'learning_rate': 0.11567448007363348, 'n_estimators': 608, 'max_depth': 3, 'min_child_weight': 1}. Best is trial 24 with value: 0.802688759745469.\n",
      "[I 2025-07-25 23:34:31,763] Trial 29 finished with value: 0.7983519735501369 and parameters: {'lambda': 1.449794875560588e-07, 'alpha': 1.611642571956379e-08, 'colsample_bytree': 0.44393822654467896, 'subsample': 0.8948364343265818, 'learning_rate': 0.27605881784033987, 'n_estimators': 215, 'max_depth': 3, 'min_child_weight': 2}. Best is trial 24 with value: 0.802688759745469.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Optuna チューニング結果 (XGBoost) ---\n",
      "最適なパラメータ: {'lambda': 2.1951709666388663e-08, 'alpha': 8.547169971217096e-08, 'colsample_bytree': 0.5521475730416037, 'subsample': 0.8615268396596035, 'learning_rate': 0.11366011585971071, 'n_estimators': 272, 'max_depth': 4, 'min_child_weight': 23}\n",
      "最適なCVスコア (Accuracy): 0.802688759745469\n",
      "\n",
      "XGBoost モデルを全データで学習中...\n",
      "--- XGBoost モデル学習完了 ---\n",
      "\n",
      "--- XGBoost テストデータ予測完了 ---\n",
      "予測された Transported の分布:\n",
      "True     2224\n",
      "False    2053\n",
      "Name: count, dtype: int64\n",
      "\n",
      "XGBoost の予測確率を保存しました: ../outputs/oof_xgb_proba.npy\n",
      "\n",
      "--- セル4完了 ---\n",
      "次のステップ: 他のモデル (例: CatBoost) や アンサンブル\n"
     ]
    }
   ],
   "source": [
    "# セル 4: XGBoost モデルの構築と予測\n",
    "\n",
    "# --- 1. 必要なライブラリのインポート ---\n",
    "import xgboost as xgb\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd # pd.Series で使用\n",
    "\n",
    "# --- 2. XGBoost 用の Optuna 目的関数の定義 ---\n",
    "def objective_xgb(trial):\n",
    "    \"\"\"\n",
    "    Optuna の目的関数 (XGBoost 用)。パラメータを提案し、StratifiedGroupKFold での CV スコア (Accuracy) を返す。\n",
    "    \"\"\"\n",
    "    # XGBoost パラメータの提案\n",
    "    param = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss', # Optuna 内部で最小化する指標\n",
    "        'booster': 'gbtree',\n",
    "        # 'tree_method': 'gpu_hist', # GPU を使用する場合はコメントを外す\n",
    "        'lambda': trial.suggest_float('lambda', 1e-8, 10.0, log=True), # L2 正則化\n",
    "        'alpha': trial.suggest_float('alpha', 1e-8, 10.0, log=True), # L1 正則化\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.4, 1.0), # 列のサブサンプリング\n",
    "        'subsample': trial.suggest_float('subsample', 0.4, 1.0), # 訓練データの行のサブサンプリング\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True), # 学習率\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000), # 木の数 (iteration数)\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12), # 木の深さ\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300), # 子ノードに必要な重みの最小値\n",
    "    }\n",
    "    \n",
    "    # StratifiedGroupKFold の設定 (LightGBM と同じ)\n",
    "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # CVスコアを格納するリスト\n",
    "    cv_scores = []\n",
    "    \n",
    "    # StratifiedGroupKFold で分割\n",
    "    for fold, (train_idx, val_idx) in enumerate(sgkf.split(X_train_processed, y_train, groups=groups_int)):\n",
    "        # データ分割\n",
    "        X_train_fold = X_train_processed.iloc[train_idx]\n",
    "        y_train_fold = y_train.iloc[train_idx]\n",
    "        X_val_fold = X_train_processed.iloc[val_idx]\n",
    "        y_val_fold = y_train.iloc[val_idx]\n",
    "\n",
    "        # XGBoost モデルのインスタンス作成\n",
    "        model = xgb.XGBClassifier(**param, random_state=42, verbosity=0)\n",
    "        \n",
    "        # --- 修正箇所 ---\n",
    "        # early_stopping を使わず、n_estimators をチューニングに任せる\n",
    "        # eval_set は指定可能だが、early_stopping_rounds と callbacks は使わない\n",
    "        # --- 修正ここまで ---\n",
    "\n",
    "        # モデル学習\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            # eval_set=[(X_val_fold, y_val_fold)], # 評価セットの指定は可能\n",
    "            verbose=False\n",
    "            # early_stopping_rounds=100, # 削除\n",
    "            # callbacks=[...] # 削除\n",
    "        )\n",
    "\n",
    "        # 予測 (確率)\n",
    "        y_pred_proba = model.predict_proba(X_val_fold)[:, 1] # True の確率を取得\n",
    "        # 確率をラベルに変換 (0.5 以上を True)\n",
    "        y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "        # Accuracy を計算\n",
    "        acc = accuracy_score(y_val_fold, y_pred)\n",
    "        cv_scores.append(acc)\n",
    "\n",
    "    # 平均CVスコアを返す (Accuracy を最大化)\n",
    "    mean_cv_score = np.mean(cv_scores)\n",
    "    return mean_cv_score\n",
    "\n",
    "# --- 3. Optuna による XGBoost のハイパーパラメーターチューニング ---\n",
    "study_xgb = optuna.create_study(direction='maximize') # Accuracy を最大化\n",
    "print(\"Optuna チューニングを開始します (XGBoost)...\")\n",
    "# 試行回数を制限 (例: 30回) して実行時間をコントロール\n",
    "study_xgb.optimize(objective_xgb, n_trials=30) # 実際には50~100程度が望ましい\n",
    "\n",
    "print(\"\\n--- Optuna チューニング結果 (XGBoost) ---\")\n",
    "print(\"最適なパラメータ:\", study_xgb.best_params)\n",
    "print(\"最適なCVスコア (Accuracy):\", study_xgb.best_value)\n",
    "\n",
    "# --- 4. 最適パラメータで XGBoost モデルを学習 ---\n",
    "best_params_xgb = study_xgb.best_params\n",
    "# best_params_xgb['tree_method'] = 'gpu_hist' # GPU 使用時\n",
    "\n",
    "# 全データで学習\n",
    "final_model_xgb = xgb.XGBClassifier(**best_params_xgb, random_state=42, verbosity=1)\n",
    "\n",
    "print(\"\\nXGBoost モデルを全データで学習中...\")\n",
    "final_model_xgb.fit(X_train_processed, y_train)\n",
    "print(\"--- XGBoost モデル学習完了 ---\")\n",
    "\n",
    "# --- 5. テストデータへの予測 ---\n",
    "# 確率で予測\n",
    "y_test_pred_proba_xgb = final_model_xgb.predict_proba(X_test_processed)[:, 1] # True の確率\n",
    "# 0.5 を閾値としてラベルに変換\n",
    "y_test_pred_xgb = (y_test_pred_proba_xgb >= 0.5).astype(bool)\n",
    "\n",
    "print(\"\\n--- XGBoost テストデータ予測完了 ---\")\n",
    "print(f\"予測された Transported の分布:\")\n",
    "print(pd.Series(y_test_pred_xgb).value_counts())\n",
    "\n",
    "# --- 6. XGBoost の予測結果を保存 (アンサンブル用) ---\n",
    "# 次のアンサンブルステップで使うために、確率も保存しておく\n",
    "np.save('../outputs/oof_xgb_proba.npy', y_test_pred_proba_xgb) # 今回は OOF ではないが、便宜上\n",
    "print(\"\\nXGBoost の予測確率を保存しました: ../outputs/oof_xgb_proba.npy\")\n",
    "\n",
    "print(\"\\n--- セル4完了 ---\")\n",
    "print(\"次のステップ: 他のモデル (例: CatBoost) や アンサンブル\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1e866c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 23:37:29,535] A new study created in memory with name: no-name-aa99ec51-5cf9-4803-9452-f41675b18cf1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna チューニングを開始します (CatBoost)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 23:37:32,312] Trial 0 finished with value: 0.8089854374228915 and parameters: {'iterations': 613, 'depth': 4, 'learning_rate': 0.22496430236300835, 'l2_leaf_reg': 0.036720348045743006, 'random_strength': 0.1483146157343797, 'bootstrap_type': 'Bernoulli', 'subsample': 0.5046107332572124}. Best is trial 0 with value: 0.8089854374228915.\n",
      "[I 2025-07-25 23:37:41,047] Trial 1 finished with value: 0.8051665642188954 and parameters: {'iterations': 242, 'depth': 9, 'learning_rate': 0.10855081672517695, 'l2_leaf_reg': 0.006212426086251139, 'random_strength': 5.906121210869269, 'bootstrap_type': 'MVS'}. Best is trial 0 with value: 0.8089854374228915.\n",
      "[I 2025-07-25 23:37:46,289] Trial 2 finished with value: 0.7501306639449714 and parameters: {'iterations': 618, 'depth': 7, 'learning_rate': 0.0010143217479284365, 'l2_leaf_reg': 1.979819661537661e-06, 'random_strength': 6.121966623881717, 'bootstrap_type': 'Bernoulli', 'subsample': 0.5163786149933159}. Best is trial 0 with value: 0.8089854374228915.\n",
      "[I 2025-07-25 23:37:54,003] Trial 3 finished with value: 0.8101664682625117 and parameters: {'iterations': 705, 'depth': 9, 'learning_rate': 0.045861156870091034, 'l2_leaf_reg': 0.009773587410742987, 'random_strength': 0.00013329782978667993, 'bootstrap_type': 'MVS'}. Best is trial 3 with value: 0.8101664682625117.\n",
      "[I 2025-07-25 23:38:01,626] Trial 4 finished with value: 0.7879741901993157 and parameters: {'iterations': 810, 'depth': 8, 'learning_rate': 0.005508665919241129, 'l2_leaf_reg': 2.499561675040662e-06, 'random_strength': 1.3236534207522414e-08, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 5.045575258741868}. Best is trial 3 with value: 0.8101664682625117.\n",
      "[I 2025-07-25 23:38:10,424] Trial 5 finished with value: 0.7942163224247123 and parameters: {'iterations': 879, 'depth': 6, 'learning_rate': 0.0028758894661559077, 'l2_leaf_reg': 4.985118306041783e-07, 'random_strength': 1.6078160992165116e-08, 'bootstrap_type': 'Bernoulli', 'subsample': 0.44411527326270517}. Best is trial 3 with value: 0.8101664682625117.\n",
      "[I 2025-07-25 23:38:17,343] Trial 6 finished with value: 0.8088386998011577 and parameters: {'iterations': 722, 'depth': 3, 'learning_rate': 0.10491397524413211, 'l2_leaf_reg': 5.086490549914162e-07, 'random_strength': 1.4307009163117488e-05, 'bootstrap_type': 'MVS'}. Best is trial 3 with value: 0.8101664682625117.\n",
      "[I 2025-07-25 23:38:23,259] Trial 7 finished with value: 0.8079045379843931 and parameters: {'iterations': 442, 'depth': 3, 'learning_rate': 0.14819575518608336, 'l2_leaf_reg': 4.031034910844584e-07, 'random_strength': 6.164396431773451, 'bootstrap_type': 'Bernoulli', 'subsample': 0.16819196166886574}. Best is trial 3 with value: 0.8101664682625117.\n",
      "[I 2025-07-25 23:38:33,318] Trial 8 finished with value: 0.8004916808581101 and parameters: {'iterations': 892, 'depth': 5, 'learning_rate': 0.007558824169175431, 'l2_leaf_reg': 1.597505006959202, 'random_strength': 0.012672120426133268, 'bootstrap_type': 'Bernoulli', 'subsample': 0.32732048318667806}. Best is trial 3 with value: 0.8101664682625117.\n",
      "[I 2025-07-25 23:38:37,471] Trial 9 finished with value: 0.8109561563824595 and parameters: {'iterations': 259, 'depth': 5, 'learning_rate': 0.19902440711518996, 'l2_leaf_reg': 0.0014464899634543868, 'random_strength': 0.002370984978577848, 'bootstrap_type': 'Bernoulli', 'subsample': 0.6456137611065956}. Best is trial 9 with value: 0.8109561563824595.\n",
      "[I 2025-07-25 23:38:43,046] Trial 10 finished with value: 0.8107926842987103 and parameters: {'iterations': 220, 'depth': 6, 'learning_rate': 0.024333795559733275, 'l2_leaf_reg': 0.00011302330404623404, 'random_strength': 1.8450871043579168e-05, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.16284976321067468}. Best is trial 9 with value: 0.8109561563824595.\n",
      "[I 2025-07-25 23:38:46,013] Trial 11 finished with value: 0.8052619700059971 and parameters: {'iterations': 120, 'depth': 6, 'learning_rate': 0.026652354084138956, 'l2_leaf_reg': 0.0001542032452088651, 'random_strength': 3.312270604784891e-06, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.04318015931786906}. Best is trial 9 with value: 0.8109561563824595.\n",
      "[I 2025-07-25 23:38:52,173] Trial 12 finished with value: 0.8117705662641012 and parameters: {'iterations': 373, 'depth': 5, 'learning_rate': 0.04285849683969768, 'l2_leaf_reg': 0.00012150236314119081, 'random_strength': 0.0026363805437634425, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 0.29064658237298424}. Best is trial 12 with value: 0.8117705662641012.\n",
      "[I 2025-07-25 23:38:58,831] Trial 13 finished with value: 0.7984498630707759 and parameters: {'iterations': 413, 'depth': 5, 'learning_rate': 0.05791557079481549, 'l2_leaf_reg': 3.850780778189134e-05, 'random_strength': 0.00722450464051014, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 9.333285217758874}. Best is trial 12 with value: 0.8117705662641012.\n",
      "[I 2025-07-25 23:39:05,240] Trial 14 finished with value: 0.7961299500961809 and parameters: {'iterations': 392, 'depth': 4, 'learning_rate': 0.012999477964834371, 'l2_leaf_reg': 1.3461118140371296e-08, 'random_strength': 0.001808913079319303, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 3.8041681640073164}. Best is trial 12 with value: 0.8117705662641012.\n",
      "[I 2025-07-25 23:39:08,651] Trial 15 finished with value: 0.8118619165423091 and parameters: {'iterations': 274, 'depth': 5, 'learning_rate': 0.256275861528107, 'l2_leaf_reg': 0.4797676532089413, 'random_strength': 0.1317696523974742, 'bootstrap_type': 'Bernoulli', 'subsample': 0.9356765819897643}. Best is trial 15 with value: 0.8118619165423091.\n",
      "[I 2025-07-25 23:39:15,980] Trial 16 finished with value: 0.8105533981770151 and parameters: {'iterations': 481, 'depth': 7, 'learning_rate': 0.052861304658370384, 'l2_leaf_reg': 2.046208864602434, 'random_strength': 0.1352108898846539, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 3.6382502472578135}. Best is trial 15 with value: 0.8118619165423091.\n",
      "[I 2025-07-25 23:39:19,526] Trial 17 finished with value: 0.8108995707106013 and parameters: {'iterations': 332, 'depth': 4, 'learning_rate': 0.28074779794373655, 'l2_leaf_reg': 0.12987745569153789, 'random_strength': 0.13241029460683834, 'bootstrap_type': 'Bernoulli', 'subsample': 0.9845638303636762}. Best is trial 15 with value: 0.8118619165423091.\n",
      "[I 2025-07-25 23:39:28,045] Trial 18 finished with value: 0.7935483293619576 and parameters: {'iterations': 128, 'depth': 10, 'learning_rate': 0.08173269400230831, 'l2_leaf_reg': 6.893336118547594, 'random_strength': 0.00020086610072558085, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 6.9701429553558025}. Best is trial 15 with value: 0.8118619165423091.\n",
      "[I 2025-07-25 23:39:36,560] Trial 19 finished with value: 0.8088787753070845 and parameters: {'iterations': 541, 'depth': 5, 'learning_rate': 0.013290780026358782, 'l2_leaf_reg': 0.2572166060925488, 'random_strength': 2.3106372326425976e-07, 'bootstrap_type': 'MVS'}. Best is trial 15 with value: 0.8118619165423091.\n",
      "[I 2025-07-25 23:39:41,747] Trial 20 finished with value: 0.8057675186063282 and parameters: {'iterations': 325, 'depth': 4, 'learning_rate': 0.030261989094324055, 'l2_leaf_reg': 0.0009894310260286031, 'random_strength': 0.6781364819871974, 'bootstrap_type': 'Bayesian', 'bagging_temperature': 2.1442467093875157}. Best is trial 15 with value: 0.8118619165423091.\n",
      "[I 2025-07-25 23:39:45,293] Trial 21 finished with value: 0.8133362587810495 and parameters: {'iterations': 228, 'depth': 5, 'learning_rate': 0.18703658353095073, 'l2_leaf_reg': 0.0015813751550870004, 'random_strength': 0.0027365162272571797, 'bootstrap_type': 'Bernoulli', 'subsample': 0.8645485289904373}. Best is trial 21 with value: 0.8133362587810495.\n",
      "[I 2025-07-25 23:39:48,425] Trial 22 finished with value: 0.8051512055568073 and parameters: {'iterations': 326, 'depth': 6, 'learning_rate': 0.2934970172922126, 'l2_leaf_reg': 1.89673408727717e-05, 'random_strength': 0.03029275373666943, 'bootstrap_type': 'Bernoulli', 'subsample': 0.9645900752748685}. Best is trial 21 with value: 0.8133362587810495.\n",
      "[I 2025-07-25 23:39:52,255] Trial 23 finished with value: 0.8107000024126553 and parameters: {'iterations': 195, 'depth': 5, 'learning_rate': 0.1542521958386977, 'l2_leaf_reg': 0.0010907338437572852, 'random_strength': 0.0007856655397078035, 'bootstrap_type': 'Bernoulli', 'subsample': 0.7742797497071566}. Best is trial 21 with value: 0.8133362587810495.\n",
      "[I 2025-07-25 23:39:56,816] Trial 24 finished with value: 0.812150191343264 and parameters: {'iterations': 303, 'depth': 7, 'learning_rate': 0.0905234666821123, 'l2_leaf_reg': 0.02632980357141096, 'random_strength': 0.745485842503681, 'bootstrap_type': 'Bernoulli', 'subsample': 0.7886296054854184}. Best is trial 21 with value: 0.8133362587810495.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Optuna チューニング結果 (CatBoost) ---\n",
      "最適なパラメータ: {'iterations': 228, 'depth': 5, 'learning_rate': 0.18703658353095073, 'l2_leaf_reg': 0.0015813751550870004, 'random_strength': 0.0027365162272571797, 'bootstrap_type': 'Bernoulli', 'subsample': 0.8645485289904373}\n",
      "最適なCVスコア (Accuracy): 0.8133362587810495\n",
      "\n",
      "CatBoost モデルを全データで学習中...\n",
      "--- CatBoost モデル学習完了 ---\n",
      "\n",
      "--- CatBoost テストデータ予測完了 ---\n",
      "予測された Transported の分布:\n",
      "True     2197\n",
      "False    2080\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CatBoost の予測確率を保存しました: ../outputs/oof_cat_proba.npy\n",
      "\n",
      "--- セル5完了 ---\n",
      "次のステップ: 他のモデル (例: HistGradientBoostingClassifier) や アンサンブル\n"
     ]
    }
   ],
   "source": [
    "# セル 5: CatBoost モデルの構築と予測\n",
    "\n",
    "# --- 1. 必要なライブラリのインポート ---\n",
    "import catboost as cb\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import pandas as pd # pd.Series で使用\n",
    "\n",
    "# --- 2. CatBoost 用の Optuna 目的関数の定義 ---\n",
    "def objective_cat(trial):\n",
    "    \"\"\"\n",
    "    Optuna の目的関数 (CatBoost 用)。パラメータを提案し、StratifiedGroupKFold での CV スコア (Accuracy) を返す。\n",
    "    \"\"\"\n",
    "    # CatBoost パラメータの基本部分を提案\n",
    "    param = {\n",
    "        'objective': 'Logloss', # 二値分類\n",
    "        'eval_metric': 'Accuracy', # 評価指標\n",
    "        # 'task_type': \"GPU\", # GPU を使用する場合はコメントを外す\n",
    "        'devices': '0', # GPU デバイス番号 (GPU使用時)\n",
    "        'iterations': trial.suggest_int('iterations', 100, 1000), # 木の数\n",
    "        'depth': trial.suggest_int('depth', 3, 10), # 木の深さ\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True), # 学習率\n",
    "        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1e-8, 10.0, log=True), # L2 正則化係数\n",
    "        'random_strength': trial.suggest_float('random_strength', 1e-8, 10.0, log=True), # ランダム化の強さ\n",
    "        'od_type': 'Iter', # overfitting detector type\n",
    "        'od_wait': 100, # overfitting detector が待機するラウンド数\n",
    "        'verbose': False, # 学習ログを非表示\n",
    "        'random_seed': 42,\n",
    "    }\n",
    "    \n",
    "    # bootstrap_type を選択\n",
    "    bootstrap_type = trial.suggest_categorical('bootstrap_type', ['Bayesian', 'Bernoulli', 'MVS'])\n",
    "    param['bootstrap_type'] = bootstrap_type\n",
    "    \n",
    "    # bootstrap_type に応じて追加パラメータを設定 (完全に分離)\n",
    "    if bootstrap_type == 'Bayesian':\n",
    "        param['bagging_temperature'] = trial.suggest_float('bagging_temperature', 0.0, 10.0)\n",
    "    elif bootstrap_type == 'Bernoulli':\n",
    "        param['subsample'] = trial.suggest_float('subsample', 0.1, 1.0)\n",
    "    # MVS の場合は追加パラメータなし\n",
    "\n",
    "    # StratifiedGroupKFold の設定 (LightGBM/XGBoost と同じ)\n",
    "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # CVスコアを格納するリスト\n",
    "    cv_scores = []\n",
    "    \n",
    "    # StratifiedGroupKFold で分割\n",
    "    for fold, (train_idx, val_idx) in enumerate(sgkf.split(X_train_processed, y_train, groups=groups_int)):\n",
    "        # データ分割\n",
    "        X_train_fold = X_train_processed.iloc[train_idx]\n",
    "        y_train_fold = y_train.iloc[train_idx]\n",
    "        X_val_fold = X_train_processed.iloc[val_idx]\n",
    "        y_val_fold = y_train.iloc[val_idx]\n",
    "\n",
    "        # CatBoost モデルのインスタンス作成\n",
    "        model = cb.CatBoostClassifier(**param)\n",
    "        \n",
    "        # モデル学習 (eval_set を指定して overfitting detector を有効化)\n",
    "        model.fit(\n",
    "            X_train_fold, y_train_fold,\n",
    "            eval_set=[(X_val_fold, y_val_fold)],\n",
    "            early_stopping_rounds=100, # od_wait と組み合わせて使用\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        # 予測 (確率)\n",
    "        y_pred_proba = model.predict_proba(X_val_fold)[:, 1] # True の確率を取得\n",
    "        # 確率をラベルに変換 (0.5 以上を True)\n",
    "        y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "        # Accuracy を計算\n",
    "        acc = accuracy_score(y_val_fold, y_pred)\n",
    "        cv_scores.append(acc)\n",
    "\n",
    "    # 平均CVスコアを返す (Accuracy を最大化)\n",
    "    mean_cv_score = np.mean(cv_scores)\n",
    "    return mean_cv_score\n",
    "\n",
    "# --- 3. Optuna による CatBoost のハイパーパラメーターチューニング ---\n",
    "study_cat = optuna.create_study(direction='maximize') # Accuracy を最大化\n",
    "print(\"Optuna チューニングを開始します (CatBoost)...\")\n",
    "# 試行回数を制限 (例: 20~30回) して実行時間をコントロール (CatBoost は少し重いので)\n",
    "study_cat.optimize(objective_cat, n_trials=25) # 実際には50~100程度が望ましいが、ここでは25回\n",
    "\n",
    "print(\"\\n--- Optuna チューニング結果 (CatBoost) ---\")\n",
    "print(\"最適なパラメータ:\", study_cat.best_params)\n",
    "print(\"最適なCVスコア (Accuracy):\", study_cat.best_value)\n",
    "\n",
    "# --- 4. 最適パラメータで CatBoost モデルを学習 ---\n",
    "best_params_cat = study_cat.best_params\n",
    "# best_params_cat['task_type'] = \"GPU\" # GPU 使用時\n",
    "# best_params_cat['devices'] = '0' # GPU 使用時\n",
    "\n",
    "# 全データで学習 (overfitting detector は使用しない、またはホールドアウトで設定)\n",
    "# od_type と od_wait は学習時にも指定する必要がある場合があるが、最終学習では省略可能\n",
    "final_model_cat = cb.CatBoostClassifier(\n",
    "    **best_params_cat,\n",
    "    verbose=False,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "print(\"\\nCatBoost モデルを全データで学習中...\")\n",
    "final_model_cat.fit(X_train_processed, y_train)\n",
    "print(\"--- CatBoost モデル学習完了 ---\")\n",
    "\n",
    "# --- 5. テストデータへの予測 ---\n",
    "# 確率で予測\n",
    "y_test_pred_proba_cat = final_model_cat.predict_proba(X_test_processed)[:, 1] # True の確率\n",
    "# 0.5 を閾値としてラベルに変換\n",
    "y_test_pred_cat = (y_test_pred_proba_cat >= 0.5).astype(bool)\n",
    "\n",
    "print(\"\\n--- CatBoost テストデータ予測完了 ---\")\n",
    "print(f\"予測された Transported の分布:\")\n",
    "print(pd.Series(y_test_pred_cat).value_counts())\n",
    "\n",
    "# --- 6. CatBoost の予測結果を保存 (アンサンブル用) ---\n",
    "# 次のアンサンブルステップで使うために、確率も保存しておく\n",
    "np.save('../outputs/oof_cat_proba.npy', y_test_pred_proba_cat) # 今回は OOF ではないが、便宜上\n",
    "print(\"\\nCatBoost の予測確率を保存しました: ../outputs/oof_cat_proba.npy\")\n",
    "\n",
    "print(\"\\n--- セル5完了 ---\")\n",
    "print(\"次のステップ: 他のモデル (例: HistGradientBoostingClassifier) や アンサンブル\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "389d08e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 23:42:35,399] A new study created in memory with name: no-name-6e743167-40fc-4020-89c3-628f6fe93d1b\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna チューニングを開始します (HistGradientBoostingClassifier)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 23:42:40,073] Trial 0 finished with value: 0.8052881825169355 and parameters: {'learning_rate': 0.011120093135150428, 'max_iter': 533, 'max_depth': 12, 'l2_regularization': 0.0022642721301332188}. Best is trial 0 with value: 0.8052881825169355.\n",
      "[I 2025-07-25 23:42:47,396] Trial 1 finished with value: 0.797137527921844 and parameters: {'learning_rate': 0.0014647298840029487, 'max_iter': 722, 'max_depth': 8, 'l2_regularization': 0.007195799265813361}. Best is trial 0 with value: 0.8052881825169355.\n",
      "[I 2025-07-25 23:42:51,415] Trial 2 finished with value: 0.7962083433682471 and parameters: {'learning_rate': 0.0028096192830954964, 'max_iter': 300, 'max_depth': 8, 'l2_regularization': 0.05161328841015196}. Best is trial 0 with value: 0.8052881825169355.\n",
      "[I 2025-07-25 23:42:54,686] Trial 3 finished with value: 0.8053460406643289 and parameters: {'learning_rate': 0.02952584900641574, 'max_iter': 565, 'max_depth': 9, 'l2_regularization': 0.06895299621458444}. Best is trial 3 with value: 0.8053460406643289.\n",
      "[I 2025-07-25 23:42:56,751] Trial 4 finished with value: 0.8037009269290541 and parameters: {'learning_rate': 0.0751955825731334, 'max_iter': 787, 'max_depth': 8, 'l2_regularization': 0.007730046858619194}. Best is trial 3 with value: 0.8053460406643289.\n",
      "[I 2025-07-25 23:42:58,395] Trial 5 finished with value: 0.8014933064131073 and parameters: {'learning_rate': 0.14291574993537556, 'max_iter': 497, 'max_depth': 6, 'l2_regularization': 0.01933976979877337}. Best is trial 3 with value: 0.8053460406643289.\n",
      "[I 2025-07-25 23:43:04,366] Trial 6 finished with value: 0.7961987796805917 and parameters: {'learning_rate': 0.0014830103849485998, 'max_iter': 489, 'max_depth': 8, 'l2_regularization': 3.5311575337271064e-06}. Best is trial 3 with value: 0.8053460406643289.\n",
      "[I 2025-07-25 23:43:07,694] Trial 7 finished with value: 0.7994437225846268 and parameters: {'learning_rate': 0.006915592880698986, 'max_iter': 253, 'max_depth': 7, 'l2_regularization': 0.11403205584148483}. Best is trial 3 with value: 0.8053460406643289.\n",
      "[I 2025-07-25 23:43:10,598] Trial 8 finished with value: 0.7684920908207906 and parameters: {'learning_rate': 0.0011598625861253572, 'max_iter': 615, 'max_depth': 3, 'l2_regularization': 1.8195084387104885e-08}. Best is trial 3 with value: 0.8053460406643289.\n",
      "[I 2025-07-25 23:43:12,565] Trial 9 finished with value: 0.803897088988653 and parameters: {'learning_rate': 0.06666579515641652, 'max_iter': 524, 'max_depth': 4, 'l2_regularization': 0.0020889493327713123}. Best is trial 3 with value: 0.8053460406643289.\n",
      "[I 2025-07-25 23:43:16,796] Trial 10 finished with value: 0.8050552162099344 and parameters: {'learning_rate': 0.027919590134185105, 'max_iter': 978, 'max_depth': 12, 'l2_regularization': 9.086379389950947}. Best is trial 3 with value: 0.8053460406643289.\n",
      "[I 2025-07-25 23:43:18,604] Trial 11 finished with value: 0.7995201022345994 and parameters: {'learning_rate': 0.012697498336346333, 'max_iter': 103, 'max_depth': 12, 'l2_regularization': 5.5313281572608454e-05}. Best is trial 3 with value: 0.8053460406643289.\n",
      "[I 2025-07-25 23:43:22,531] Trial 12 finished with value: 0.8047020600100572 and parameters: {'learning_rate': 0.02245819452711596, 'max_iter': 362, 'max_depth': 10, 'l2_regularization': 8.487953845238396}. Best is trial 3 with value: 0.8053460406643289.\n",
      "[I 2025-07-25 23:43:29,749] Trial 13 finished with value: 0.8045090499905567 and parameters: {'learning_rate': 0.0058080053486682754, 'max_iter': 699, 'max_depth': 10, 'l2_regularization': 4.852745037147924e-05}. Best is trial 3 with value: 0.8053460406643289.\n",
      "[I 2025-07-25 23:43:32,333] Trial 14 finished with value: 0.8053286355410982 and parameters: {'learning_rate': 0.04010274419065507, 'max_iter': 858, 'max_depth': 10, 'l2_regularization': 0.5042546162730964}. Best is trial 3 with value: 0.8053460406643289.\n",
      "[I 2025-07-25 23:43:33,785] Trial 15 finished with value: 0.8002601571833395 and parameters: {'learning_rate': 0.26367288437616176, 'max_iter': 925, 'max_depth': 10, 'l2_regularization': 0.9955355243664723}. Best is trial 3 with value: 0.8053460406643289.\n",
      "[I 2025-07-25 23:43:36,499] Trial 16 finished with value: 0.8054180486455309 and parameters: {'learning_rate': 0.03466690134931694, 'max_iter': 836, 'max_depth': 10, 'l2_regularization': 0.2163254731996442}. Best is trial 16 with value: 0.8054180486455309.\n",
      "[I 2025-07-25 23:43:39,009] Trial 17 finished with value: 0.8048467303430902 and parameters: {'learning_rate': 0.04190804401258447, 'max_iter': 815, 'max_depth': 9, 'l2_regularization': 0.47419274558421864}. Best is trial 16 with value: 0.8054180486455309.\n",
      "[I 2025-07-25 23:43:40,507] Trial 18 finished with value: 0.8043800445646798 and parameters: {'learning_rate': 0.11932286095309987, 'max_iter': 641, 'max_depth': 6, 'l2_regularization': 0.00034288014498013087}. Best is trial 16 with value: 0.8054180486455309.\n",
      "[I 2025-07-25 23:43:44,884] Trial 19 finished with value: 0.7995606313571277 and parameters: {'learning_rate': 0.005619901659465748, 'max_iter': 374, 'max_depth': 11, 'l2_regularization': 9.896069637580032e-07}. Best is trial 16 with value: 0.8054180486455309.\n",
      "[I 2025-07-25 23:43:49,099] Trial 20 finished with value: 0.8039316223233224 and parameters: {'learning_rate': 0.01877204423325593, 'max_iter': 898, 'max_depth': 9, 'l2_regularization': 1.1242350635830105}. Best is trial 16 with value: 0.8054180486455309.\n",
      "[I 2025-07-25 23:43:51,413] Trial 21 finished with value: 0.8046332223235023 and parameters: {'learning_rate': 0.04731470652771929, 'max_iter': 828, 'max_depth': 11, 'l2_regularization': 0.11921776615429303}. Best is trial 16 with value: 0.8054180486455309.\n",
      "[I 2025-07-25 23:43:54,213] Trial 22 finished with value: 0.8046204952858662 and parameters: {'learning_rate': 0.03471284148802221, 'max_iter': 996, 'max_depth': 9, 'l2_regularization': 0.4703510315978551}. Best is trial 16 with value: 0.8054180486455309.\n",
      "[I 2025-07-25 23:43:56,203] Trial 23 finished with value: 0.8032202576503042 and parameters: {'learning_rate': 0.06742209247392723, 'max_iter': 846, 'max_depth': 11, 'l2_regularization': 1.394811879008055}. Best is trial 16 with value: 0.8054180486455309.\n",
      "[I 2025-07-25 23:44:01,695] Trial 24 finished with value: 0.8053304811888145 and parameters: {'learning_rate': 0.013629497022703727, 'max_iter': 720, 'max_depth': 10, 'l2_regularization': 0.08673633070239374}. Best is trial 16 with value: 0.8054180486455309.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Optuna チューニング結果 (HistGradientBoostingClassifier) ---\n",
      "最適なパラメータ: {'learning_rate': 0.03466690134931694, 'max_iter': 836, 'max_depth': 10, 'l2_regularization': 0.2163254731996442}\n",
      "最適なCVスコア (Accuracy): 0.8054180486455309\n",
      "\n",
      "HistGradientBoostingClassifier モデルを全データで学習中...\n",
      "--- HistGradientBoostingClassifier モデル学習完了 ---\n",
      "\n",
      "--- HistGradientBoostingClassifier テストデータ予測完了 ---\n",
      "予測された Transported の分布:\n",
      "True     2186\n",
      "False    2091\n",
      "Name: count, dtype: int64\n",
      "\n",
      "HistGradientBoostingClassifier の予測確率を保存しました: ../outputs/oof_hgb_proba.npy\n",
      "\n",
      "--- セル6完了 ---\n",
      "次のステップ: 他のモデル (例: RandomForestClassifier) や アンサンブルの準備\n"
     ]
    }
   ],
   "source": [
    "# セル 6: HistGradientBoostingClassifier モデルの構築と予測\n",
    "\n",
    "# --- 1. 必要なライブラリのインポート ---\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# --- 2. HistGradientBoostingClassifier 用の Optuna 目的関数の定義 ---\n",
    "def objective_hgb(trial):\n",
    "    \"\"\"\n",
    "    Optuna の目的関数 (HistGradientBoostingClassifier 用)。\n",
    "    パラメータを提案し、StratifiedGroupKFold での CV スコア (Accuracy) を返す。\n",
    "    \"\"\"\n",
    "    # HistGradientBoostingClassifier パラメータの提案\n",
    "    param = {\n",
    "        'loss': 'log_loss', # 二値分類\n",
    "        # 'device': 'cuda', # GPU を使用する場合はコメントを外す (sklearn >= 1.5 が必要)\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.001, 0.3, log=True), # 学習率\n",
    "        'max_iter': trial.suggest_int('max_iter', 100, 1000), # 最大イテレーション数\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12), # 木の最大深さ\n",
    "        'l2_regularization': trial.suggest_float('l2_regularization', 1e-8, 10.0, log=True), # L2 正則化\n",
    "        'early_stopping': True, # 早期停止を有効化\n",
    "        'validation_fraction': 0.1, # 検証用データの割合 (early_stopping 用)\n",
    "        'n_iter_no_change': 50, # 早期停止の待ちラウンド数\n",
    "        'tol': 1e-7, # 早期停止の許容誤差\n",
    "        'random_state': 42,\n",
    "        'verbose': 0, # 学習ログを非表示\n",
    "    }\n",
    "    \n",
    "    # StratifiedGroupKFold の設定 (他のモデルと同じ)\n",
    "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # CVスコアを格納するリスト\n",
    "    cv_scores = []\n",
    "    \n",
    "    # StratifiedGroupKFold で分割 (注意: HistGradientBoosting は内部でホールドアウトしない)\n",
    "    # したがって、ここでは通常の StratifiedGroupKFold で評価する\n",
    "    for fold, (train_idx, val_idx) in enumerate(sgkf.split(X_train_processed, y_train, groups=groups_int)):\n",
    "        # データ分割\n",
    "        X_train_fold = X_train_processed.iloc[train_idx]\n",
    "        y_train_fold = y_train.iloc[train_idx]\n",
    "        X_val_fold = X_train_processed.iloc[val_idx]\n",
    "        y_val_fold = y_train.iloc[val_idx]\n",
    "\n",
    "        # HistGradientBoostingClassifier モデルのインスタンス作成\n",
    "        model = HistGradientBoostingClassifier(**param)\n",
    "        \n",
    "        # モデル学習 (early_stopping は内部で validation_fraction に基づいて処理される)\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # 予測 (確率)\n",
    "        y_pred_proba = model.predict_proba(X_val_fold)[:, 1] # True の確率を取得\n",
    "        # 確率をラベルに変換 (0.5 以上を True)\n",
    "        y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "        # Accuracy を計算\n",
    "        acc = accuracy_score(y_val_fold, y_pred)\n",
    "        cv_scores.append(acc)\n",
    "\n",
    "    # 平均CVスコアを返す (Accuracy を最大化)\n",
    "    mean_cv_score = np.mean(cv_scores)\n",
    "    return mean_cv_score\n",
    "\n",
    "# --- 3. Optuna による HistGradientBoostingClassifier のハイパーパラメーターチューニング ---\n",
    "study_hgb = optuna.create_study(direction='maximize') # Accuracy を最大化\n",
    "print(\"Optuna チューニングを開始します (HistGradientBoostingClassifier)...\")\n",
    "# 試行回数を制限 (例: 20~30回)\n",
    "study_hgb.optimize(objective_hgb, n_trials=25) # 実際には50~100程度が望ましいが、ここでは25回\n",
    "\n",
    "print(\"\\n--- Optuna チューニング結果 (HistGradientBoostingClassifier) ---\")\n",
    "print(\"最適なパラメータ:\", study_hgb.best_params)\n",
    "print(\"最適なCVスコア (Accuracy):\", study_hgb.best_value)\n",
    "\n",
    "# --- 4. 最適パラメータで HistGradientBoostingClassifier モデルを学習 ---\n",
    "best_params_hgb = study_hgb.best_params\n",
    "# best_params_hgb['device'] = 'cuda' # GPU 使用時\n",
    "best_params_hgb['early_stopping'] = False # 最終学習では早期停止を無効化し、全データを使う\n",
    "best_params_hgb['verbose'] = 0\n",
    "best_params_hgb['random_state'] = 42\n",
    "\n",
    "# 全データで学習\n",
    "final_model_hgb = HistGradientBoostingClassifier(**best_params_hgb)\n",
    "\n",
    "print(\"\\nHistGradientBoostingClassifier モデルを全データで学習中...\")\n",
    "final_model_hgb.fit(X_train_processed, y_train)\n",
    "print(\"--- HistGradientBoostingClassifier モデル学習完了 ---\")\n",
    "\n",
    "# --- 5. テストデータへの予測 ---\n",
    "# 確率で予測\n",
    "y_test_pred_proba_hgb = final_model_hgb.predict_proba(X_test_processed)[:, 1] # True の確率\n",
    "# 0.5 を閾値としてラベルに変換\n",
    "y_test_pred_hgb = (y_test_pred_proba_hgb >= 0.5).astype(bool)\n",
    "\n",
    "print(\"\\n--- HistGradientBoostingClassifier テストデータ予測完了 ---\")\n",
    "print(f\"予測された Transported の分布:\")\n",
    "print(pd.Series(y_test_pred_hgb).value_counts())\n",
    "\n",
    "# --- 6. HistGradientBoostingClassifier の予測結果を保存 (アンサンブル用) ---\n",
    "# 次のアンサンブルステップで使うために、確率も保存しておく\n",
    "np.save('../outputs/oof_hgb_proba.npy', y_test_pred_proba_hgb) # 今回は OOF ではないが、便宜上\n",
    "print(\"\\nHistGradientBoostingClassifier の予測確率を保存しました: ../outputs/oof_hgb_proba.npy\")\n",
    "\n",
    "print(\"\\n--- セル6完了 ---\")\n",
    "print(\"次のステップ: 他のモデル (例: RandomForestClassifier) や アンサンブルの準備\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4229061",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 23:45:47,507] A new study created in memory with name: no-name-56610711-b64b-462a-ad5c-a605ccd2191f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optuna チューニングを開始します (RandomForestClassifier)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-07-25 23:45:49,881] Trial 0 finished with value: 0.7970889236785873 and parameters: {'n_estimators': 214, 'max_depth': 9, 'min_samples_split': 75, 'min_samples_leaf': 54, 'max_features': 0.5186479351030339, 'bootstrap': False}. Best is trial 0 with value: 0.7970889236785873.\n",
      "[I 2025-07-25 23:45:57,300] Trial 1 finished with value: 0.7958044862437375 and parameters: {'n_estimators': 783, 'max_depth': 11, 'min_samples_split': 2, 'min_samples_leaf': 42, 'max_features': 0.42191071870164465, 'bootstrap': True}. Best is trial 0 with value: 0.7970889236785873.\n",
      "[I 2025-07-25 23:46:05,986] Trial 2 finished with value: 0.7883236253203936 and parameters: {'n_estimators': 888, 'max_depth': 18, 'min_samples_split': 53, 'min_samples_leaf': 59, 'max_features': 0.19326703912015103, 'bootstrap': True}. Best is trial 0 with value: 0.7970889236785873.\n",
      "[I 2025-07-25 23:46:12,716] Trial 3 finished with value: 0.7670910730648899 and parameters: {'n_estimators': 611, 'max_depth': 3, 'min_samples_split': 49, 'min_samples_leaf': 72, 'max_features': 0.8293818796009385, 'bootstrap': True}. Best is trial 0 with value: 0.7970889236785873.\n",
      "[I 2025-07-25 23:46:22,506] Trial 4 finished with value: 0.8023457020045672 and parameters: {'n_estimators': 813, 'max_depth': 15, 'min_samples_split': 9, 'min_samples_leaf': 25, 'max_features': 0.4184988923488683, 'bootstrap': True}. Best is trial 4 with value: 0.8023457020045672.\n",
      "[I 2025-07-25 23:46:35,375] Trial 5 finished with value: 0.7919567898507069 and parameters: {'n_estimators': 927, 'max_depth': 19, 'min_samples_split': 31, 'min_samples_leaf': 58, 'max_features': 0.8574425894900569, 'bootstrap': False}. Best is trial 4 with value: 0.8023457020045672.\n",
      "[I 2025-07-25 23:46:44,438] Trial 6 finished with value: 0.798546014310253 and parameters: {'n_estimators': 627, 'max_depth': 18, 'min_samples_split': 35, 'min_samples_leaf': 34, 'max_features': 0.9347075562128438, 'bootstrap': True}. Best is trial 4 with value: 0.8023457020045672.\n",
      "[I 2025-07-25 23:46:50,815] Trial 7 finished with value: 0.7979832509300788 and parameters: {'n_estimators': 600, 'max_depth': 11, 'min_samples_split': 75, 'min_samples_leaf': 50, 'max_features': 0.36516912188775297, 'bootstrap': False}. Best is trial 4 with value: 0.8023457020045672.\n",
      "[I 2025-07-25 23:46:53,112] Trial 8 finished with value: 0.7922350463494642 and parameters: {'n_estimators': 187, 'max_depth': 8, 'min_samples_split': 5, 'min_samples_leaf': 80, 'max_features': 0.2646267268204632, 'bootstrap': False}. Best is trial 4 with value: 0.8023457020045672.\n",
      "[I 2025-07-25 23:47:02,213] Trial 9 finished with value: 0.7983162057279911 and parameters: {'n_estimators': 796, 'max_depth': 6, 'min_samples_split': 73, 'min_samples_leaf': 6, 'max_features': 0.5075750495503447, 'bootstrap': True}. Best is trial 4 with value: 0.8023457020045672.\n",
      "[I 2025-07-25 23:47:07,405] Trial 10 finished with value: 0.8012378560681434 and parameters: {'n_estimators': 356, 'max_depth': 15, 'min_samples_split': 96, 'min_samples_leaf': 17, 'max_features': 0.7006055847181485, 'bootstrap': True}. Best is trial 4 with value: 0.8023457020045672.\n",
      "[I 2025-07-25 23:47:13,205] Trial 11 finished with value: 0.8005572334562064 and parameters: {'n_estimators': 409, 'max_depth': 15, 'min_samples_split': 98, 'min_samples_leaf': 16, 'max_features': 0.7030328431552848, 'bootstrap': True}. Best is trial 4 with value: 0.8023457020045672.\n",
      "[I 2025-07-25 23:47:18,032] Trial 12 finished with value: 0.8011153022290449 and parameters: {'n_estimators': 339, 'max_depth': 15, 'min_samples_split': 97, 'min_samples_leaf': 23, 'max_features': 0.6377077743645571, 'bootstrap': True}. Best is trial 4 with value: 0.8023457020045672.\n",
      "[I 2025-07-25 23:47:24,513] Trial 13 finished with value: 0.8022388806717137 and parameters: {'n_estimators': 445, 'max_depth': 15, 'min_samples_split': 30, 'min_samples_leaf': 3, 'max_features': 0.6878292016846363, 'bootstrap': True}. Best is trial 4 with value: 0.8023457020045672.\n",
      "[I 2025-07-25 23:47:30,210] Trial 14 finished with value: 0.7924431195805987 and parameters: {'n_estimators': 490, 'max_depth': 14, 'min_samples_split': 18, 'min_samples_leaf': 97, 'max_features': 0.34763084210348383, 'bootstrap': True}. Best is trial 4 with value: 0.8023457020045672.\n",
      "[I 2025-07-25 23:47:40,019] Trial 15 finished with value: 0.8033679686288189 and parameters: {'n_estimators': 802, 'max_depth': 13, 'min_samples_split': 20, 'min_samples_leaf': 7, 'max_features': 0.5651473671734111, 'bootstrap': True}. Best is trial 15 with value: 0.8033679686288189.\n",
      "[I 2025-07-25 23:47:46,745] Trial 16 finished with value: 0.7838558732739069 and parameters: {'n_estimators': 745, 'max_depth': 13, 'min_samples_split': 21, 'min_samples_leaf': 28, 'max_features': 0.12945471181866386, 'bootstrap': True}. Best is trial 15 with value: 0.8033679686288189.\n",
      "[I 2025-07-25 23:47:56,339] Trial 17 finished with value: 0.8019143048913022 and parameters: {'n_estimators': 707, 'max_depth': 17, 'min_samples_split': 14, 'min_samples_leaf': 1, 'max_features': 0.5767354755621377, 'bootstrap': True}. Best is trial 15 with value: 0.8033679686288189.\n",
      "[I 2025-07-25 23:48:06,823] Trial 18 finished with value: 0.8001955759314618 and parameters: {'n_estimators': 991, 'max_depth': 12, 'min_samples_split': 47, 'min_samples_leaf': 34, 'max_features': 0.44184377123155205, 'bootstrap': False}. Best is trial 15 with value: 0.8033679686288189.\n",
      "[I 2025-07-25 23:48:15,955] Trial 19 finished with value: 0.8030519715316661 and parameters: {'n_estimators': 863, 'max_depth': 20, 'min_samples_split': 15, 'min_samples_leaf': 13, 'max_features': 0.2821489387321353, 'bootstrap': True}. Best is trial 15 with value: 0.8033679686288189.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Optuna チューニング結果 (RandomForestClassifier) ---\n",
      "最適なパラメータ: {'n_estimators': 802, 'max_depth': 13, 'min_samples_split': 20, 'min_samples_leaf': 7, 'max_features': 0.5651473671734111, 'bootstrap': True}\n",
      "最適なCVスコア (Accuracy): 0.8033679686288189\n",
      "\n",
      "RandomForestClassifier モデルを全データで学習中...\n",
      "--- RandomForestClassifier モデル学習完了 ---\n",
      "\n",
      "--- RandomForestClassifier テストデータ予測完了 ---\n",
      "予測された Transported の分布:\n",
      "True     2144\n",
      "False    2133\n",
      "Name: count, dtype: int64\n",
      "\n",
      "RandomForestClassifier の予測確率を保存しました: ../outputs/oof_rf_proba.npy\n",
      "\n",
      "--- セル7完了 ---\n",
      "次のステップ: アンサンブル (モデルの予測確率の平均化など)\n"
     ]
    }
   ],
   "source": [
    "# セル 7: RandomForestClassifier モデルの構築と予測\n",
    "\n",
    "# --- 1. 必要なライブラリのインポート ---\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import optuna\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# --- 2. RandomForestClassifier 用の Optuna 目的関数の定義 ---\n",
    "def objective_rf(trial):\n",
    "    \"\"\"\n",
    "    Optuna の目的関数 (RandomForestClassifier 用)。\n",
    "    パラメータを提案し、StratifiedGroupKFold での CV スコア (Accuracy) を返す。\n",
    "    \"\"\"\n",
    "    # RandomForestClassifier パラメータの提案\n",
    "    param = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000), # 決定木の数\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 20), # 木の最大深さ\n",
    "        'min_samples_split': trial.suggest_int('min_samples_split', 2, 100), # 内部ノードを分割するのに必要な最小サンプル数\n",
    "        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 100), # 葉ノードに必要な最小サンプル数\n",
    "        'max_features': trial.suggest_float('max_features', 0.1, 1.0), # 各分割で考慮する特徴量の割合\n",
    "        'bootstrap': trial.suggest_categorical('bootstrap', [True, False]), # ブートストラップサンプリングを行うか\n",
    "        # 'oob_score': True, # Out-of-Bag スコアを計算 (early stopping の代替になる可能性あり)\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1, # 並列処理\n",
    "        'verbose': 0, # 学習ログを非表示\n",
    "    }\n",
    "    \n",
    "    # StratifiedGroupKFold の設定 (他のモデルと同じ)\n",
    "    sgkf = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    \n",
    "    # CVスコアを格納するリスト\n",
    "    cv_scores = []\n",
    "    \n",
    "    # StratifiedGroupKFold で分割\n",
    "    for fold, (train_idx, val_idx) in enumerate(sgkf.split(X_train_processed, y_train, groups=groups_int)):\n",
    "        # データ分割\n",
    "        X_train_fold = X_train_processed.iloc[train_idx]\n",
    "        y_train_fold = y_train.iloc[train_idx]\n",
    "        X_val_fold = X_train_processed.iloc[val_idx]\n",
    "        y_val_fold = y_train.iloc[val_idx]\n",
    "\n",
    "        # RandomForestClassifier モデルのインスタンス作成\n",
    "        model = RandomForestClassifier(**param)\n",
    "        \n",
    "        # モデル学習\n",
    "        model.fit(X_train_fold, y_train_fold)\n",
    "\n",
    "        # 予測 (確率)\n",
    "        y_pred_proba = model.predict_proba(X_val_fold)[:, 1] # True の確率を取得\n",
    "        # 確率をラベルに変換 (0.5 以上を True)\n",
    "        y_pred = (y_pred_proba >= 0.5).astype(int)\n",
    "\n",
    "        # Accuracy を計算\n",
    "        acc = accuracy_score(y_val_fold, y_pred)\n",
    "        cv_scores.append(acc)\n",
    "\n",
    "    # 平均CVスコアを返す (Accuracy を最大化)\n",
    "    mean_cv_score = np.mean(cv_scores)\n",
    "    return mean_cv_score\n",
    "\n",
    "# --- 3. Optuna による RandomForestClassifier のハイパーパラメーターチューニング ---\n",
    "study_rf = optuna.create_study(direction='maximize') # Accuracy を最大化\n",
    "print(\"Optuna チューニングを開始します (RandomForestClassifier)...\")\n",
    "# 試行回数を制限 (例: 20~30回) RandomForest は比較的重いので\n",
    "study_rf.optimize(objective_rf, n_trials=20) # 実際には50~100程度が望ましいが、ここでは20回\n",
    "\n",
    "print(\"\\n--- Optuna チューニング結果 (RandomForestClassifier) ---\")\n",
    "print(\"最適なパラメータ:\", study_rf.best_params)\n",
    "print(\"最適なCVスコア (Accuracy):\", study_rf.best_value)\n",
    "\n",
    "# --- 4. 最適パラメータで RandomForestClassifier モデルを学習 ---\n",
    "best_params_rf = study_rf.best_params\n",
    "best_params_rf['random_state'] = 42\n",
    "best_params_rf['n_jobs'] = -1\n",
    "best_params_rf['verbose'] = 0\n",
    "\n",
    "# 全データで学習\n",
    "final_model_rf = RandomForestClassifier(**best_params_rf)\n",
    "\n",
    "print(\"\\nRandomForestClassifier モデルを全データで学習中...\")\n",
    "final_model_rf.fit(X_train_processed, y_train)\n",
    "print(\"--- RandomForestClassifier モデル学習完了 ---\")\n",
    "\n",
    "# --- 5. テストデータへの予測 ---\n",
    "# 確率で予測\n",
    "y_test_pred_proba_rf = final_model_rf.predict_proba(X_test_processed)[:, 1] # True の確率\n",
    "# 0.5 を閾値としてラベルに変換\n",
    "y_test_pred_rf = (y_test_pred_proba_rf >= 0.5).astype(bool)\n",
    "\n",
    "print(\"\\n--- RandomForestClassifier テストデータ予測完了 ---\")\n",
    "print(f\"予測された Transported の分布:\")\n",
    "print(pd.Series(y_test_pred_rf).value_counts())\n",
    "\n",
    "# --- 6. RandomForestClassifier の予測結果を保存 (アンサンブル用) ---\n",
    "# 次のアンサンブルステップで使うために、確率も保存しておく\n",
    "np.save('../outputs/oof_rf_proba.npy', y_test_pred_proba_rf) # 今回は OOF ではないが、便宜上\n",
    "print(\"\\nRandomForestClassifier の予測確率を保存しました: ../outputs/oof_rf_proba.npy\")\n",
    "\n",
    "print(\"\\n--- セル7完了 ---\")\n",
    "print(\"次のステップ: アンサンブル (モデルの予測確率の平均化など)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e22ffd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- ユーザー想定に基づくアンサンブル (重み正規化後) ---\n",
      "  LightGBM: 0.1999\n",
      "  XGBoost: 0.1992\n",
      "  CatBoost: 0.2018\n",
      "  HistGradientBoosting: 0.1998\n",
      "  RandomForest: 0.1993\n",
      "\n",
      "ユーザー想定加重平均アンサンブル予測された Transported の分布:\n",
      "True     2208\n",
      "False    2069\n",
      "Name: count, dtype: int64\n",
      "\n",
      "ユーザー想定加重平均アンサンブル提出ファイルを保存しました: ../outputs/submissions/ver5/soya_model5.csv\n",
      "\n",
      "--- 比較: 単純平均アンサンブル ---\n",
      "単純平均アンサンブル予測された Transported の分布:\n",
      "True     2209\n",
      "False    2068\n",
      "Name: count, dtype: int64\n",
      "単純平均アンサンブル提出ファイルを保存しました: ../outputs/submissions/ver5/soya_model5.csv\n",
      "\n",
      "--- セル8完了 ---\n",
      "次のステップ: Kaggleに提出してスコアを確認、またはさらに高度なアンサンブル (Optuna重み最適化, Stacking)\n"
     ]
    }
   ],
   "source": [
    "# セル 8: ユーザー想定に基づくアンサンブル (Weighted Average based on model trust/performance)\n",
    "\n",
    "# --- 1. 各モデルの予測確率を変数から取得 (既に計算済みと仮定) ---\n",
    "# y_test_pred_proba          # LightGBM (from Cell 3)\n",
    "# y_test_pred_proba_xgb      # XGBoost (from Cell 4)\n",
    "# y_test_pred_proba_cat      # CatBoost (from Cell 5)\n",
    "# y_test_pred_proba_hgb      # HistGradientBoosting (from Cell 6)\n",
    "# y_test_pred_proba_rf       # RandomForest (from Cell 7)\n",
    "\n",
    "# --- 2. 各モデルの重みを定義 (あなたの想定に基づく) ---\n",
    "# ここでは、各モデルの Optuna チューニングで得られた CV Accuracy を基準に重みを設定\n",
    "# 重みは任意に調整可能です\n",
    "model_weights = {\n",
    "    'LightGBM': 0.8055,\n",
    "    'XGBoost': 0.8027,\n",
    "    'CatBoost': 0.8133,\n",
    "    'HistGradientBoosting': 0.8054,\n",
    "    'RandomForest': 0.8034\n",
    "}\n",
    "\n",
    "# --- 3. 重みの正規化 (合計を1にする) ---\n",
    "total_weight = sum(model_weights.values())\n",
    "normalized_weights = {model: weight / total_weight for model, weight in model_weights.items()}\n",
    "\n",
    "print(\"--- ユーザー想定に基づくアンサンブル (重み正規化後) ---\")\n",
    "for model, weight in normalized_weights.items():\n",
    "    print(f\"  {model}: {weight:.4f}\")\n",
    "\n",
    "# --- 4. 加重平均 (Weighted Average) ---\n",
    "ensemble_proba_weighted_user = (\n",
    "    normalized_weights['LightGBM'] * y_test_pred_proba +\n",
    "    normalized_weights['XGBoost'] * y_test_pred_proba_xgb +\n",
    "    normalized_weights['CatBoost'] * y_test_pred_proba_cat +\n",
    "    normalized_weights['HistGradientBoosting'] * y_test_pred_proba_hgb +\n",
    "    normalized_weights['RandomForest'] * y_test_pred_proba_rf\n",
    ")\n",
    "\n",
    "# 確率をラベルに変換 (0.5 以上を True)\n",
    "y_test_pred_weighted_user = (ensemble_proba_weighted_user >= 0.5).astype(bool)\n",
    "\n",
    "print(f\"\\nユーザー想定加重平均アンサンブル予測された Transported の分布:\")\n",
    "print(pd.Series(y_test_pred_weighted_user).value_counts())\n",
    "\n",
    "# --- 5. 提出ファイルの作成 ---\n",
    "submission_weighted_user = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Transported': y_test_pred_weighted_user\n",
    "})\n",
    "submission_path_weighted_user = '../outputs/submissions/ver5/soya_model5.csv'\n",
    "submission_weighted_user.to_csv(submission_path_weighted_user, index=False)\n",
    "print(f\"\\nユーザー想定加重平均アンサンブル提出ファイルを保存しました: {submission_path_weighted_user}\")\n",
    "\n",
    "# --- 6. 比較のため、単純平均も再掲 (Optional) ---\n",
    "print(\"\\n--- 比較: 単純平均アンサンブル ---\")\n",
    "ensemble_proba_simple = (\n",
    "    y_test_pred_proba +\n",
    "    y_test_pred_proba_xgb +\n",
    "    y_test_pred_proba_cat +\n",
    "    y_test_pred_proba_hgb +\n",
    "    y_test_pred_proba_rf\n",
    ") / 5.0\n",
    "\n",
    "y_test_pred_simple = (ensemble_proba_simple >= 0.5).astype(bool)\n",
    "print(f\"単純平均アンサンブル予測された Transported の分布:\")\n",
    "print(pd.Series(y_test_pred_simple).value_counts())\n",
    "\n",
    "submission_simple = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Transported': y_test_pred_simple\n",
    "})\n",
    "submission_path_simple = '../outputs/submissions/ver5/soya_model5.csv'\n",
    "submission_simple.to_csv(submission_path_simple, index=False)\n",
    "print(f\"単純平均アンサンブル提出ファイルを保存しました: {submission_path_simple}\")\n",
    "\n",
    "print(\"\\n--- セル8完了 ---\")\n",
    "print(\"次のステップ: Kaggleに提出してスコアを確認、またはさらに高度なアンサンブル (Optuna重み最適化, Stacking)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b67ebef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CatBoost 単体モデルによる予測 ---\n",
      "学習済み CatBoost モデル: <class 'catboost.core.CatBoostClassifier'>\n",
      "CatBoost単体モデル予測された Transported の分布:\n",
      "True     2197\n",
      "False    2080\n",
      "Name: count, dtype: int64\n",
      "\n",
      "CatBoost単体モデル提出ファイルを保存しました: ../outputs/submissions/ver6/soya_model6.csv\n",
      "\n",
      "--- セル9完了 ---\n",
      "次のステップ: CatBoost単体モデルの提出ファイルを Kaggle にアップロードしてスコア確認\n"
     ]
    }
   ],
   "source": [
    "# セル 9: CatBoost単体モデルによる予測と提出\n",
    "\n",
    "# --- 1. 必要なライブラリのインポート ---\n",
    "# (ファイル操作のために os をインポート)\n",
    "import os\n",
    "\n",
    "# --- 2. モデルの確認 ---\n",
    "# final_model_cat がセル5で学習済みであることを前提とします。\n",
    "print(\"--- CatBoost 単体モデルによる予測 ---\")\n",
    "print(\"学習済み CatBoost モデル:\", type(final_model_cat))\n",
    "\n",
    "# --- 3. テストデータへの予測 (確率) ---\n",
    "# セル5で既にこの処理を行っていますが、念のため再掲します。\n",
    "# y_test_pred_proba_cat は既に存在するはずですが、\n",
    "# もし存在しない場合は以下の行のコメントを外して実行してください。\n",
    "# y_test_pred_proba_cat = final_model_cat.predict_proba(X_test_processed)[:, 1] # True の確率\n",
    "\n",
    "# --- 4. 確率をラベルに変換 ---\n",
    "# セル5で既にこの処理を行っていますが、念のため再掲します。\n",
    "y_test_pred_cat_single = (y_test_pred_proba_cat >= 0.5).astype(bool)\n",
    "\n",
    "print(f\"CatBoost単体モデル予測された Transported の分布:\")\n",
    "print(pd.Series(y_test_pred_cat_single).value_counts())\n",
    "\n",
    "# --- 5. 提出ファイルの作成 ---\n",
    "submission_cat_single = pd.DataFrame({\n",
    "    'PassengerId': test_df['PassengerId'],\n",
    "    'Transported': y_test_pred_cat_single\n",
    "})\n",
    "\n",
    "# --- 修正箇所 ---\n",
    "# 保存先ディレクトリを指定 (ver6)\n",
    "submission_dir = '../outputs/submissions/ver6/'\n",
    "\n",
    "# ディレクトリが存在しない場合は作成\n",
    "os.makedirs(submission_dir, exist_ok=True)\n",
    "\n",
    "# 提出ファイルのパスを指定\n",
    "submission_path_cat_single = os.path.join(submission_dir, 'soya_model6.csv')\n",
    "# --- 修正ここまで ---\n",
    "\n",
    "# 提出ファイルを保存\n",
    "submission_cat_single.to_csv(submission_path_cat_single, index=False)\n",
    "print(f\"\\nCatBoost単体モデル提出ファイルを保存しました: {submission_path_cat_single}\")\n",
    "\n",
    "print(\"\\n--- セル9完了 ---\")\n",
    "print(\"次のステップ: CatBoost単体モデルの提出ファイルを Kaggle にアップロードしてスコア確認\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
